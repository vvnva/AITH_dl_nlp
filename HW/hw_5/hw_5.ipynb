{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1412b309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from peft import (get_peft_model,\n",
    "                  LoraConfig,\n",
    "                  PrefixTuningConfig,\n",
    "                  TaskType,)\n",
    "from transformers import (AutoModelForSequenceClassification,\n",
    "                          AutoTokenizer,\n",
    "                          DataCollatorWithPadding,\n",
    "                          Trainer, \n",
    "                          TrainingArguments,)\n",
    "\n",
    "# Фиксация сидов для воспроизводимости\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Устройство\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31dfa0a",
   "metadata": {},
   "source": [
    "## 0. Загрузка/подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e9395df",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"dair-ai/emotion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c716f1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"google-bert/bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54ba228e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86a6457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d059bd33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names = dataset['train'].features['label'].names\n",
    "num_labels = len(label_names)\n",
    "label_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4631034d",
   "metadata": {},
   "source": [
    "## 1. Метрики до дообучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22cb1699",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21ce452c",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_metric  = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_metric.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"precision\": precision_metric.compute(predictions=preds, references=labels, average=\"macro\", zero_division=0)[\"precision\"],\n",
    "        \"recall\": recall_metric.compute(predictions=preds, references=labels, average=\"macro\", zero_division=0)[\"recall\"],\n",
    "        \"f1_macro\": f1_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e23249e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./pretrain\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_eval_batch_size=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68147177",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    eval_dataset=dataset['validation'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b505ddea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.6839704513549805,\n",
       " 'eval_model_preparation_time': 0.0,\n",
       " 'eval_accuracy': 0.151,\n",
       " 'eval_precision': 0.14487953510576054,\n",
       " 'eval_recall': 0.1618697266408435,\n",
       " 'eval_f1_macro': 0.07286239293441214,\n",
       " 'eval_runtime': 2.107,\n",
       " 'eval_samples_per_second': 949.235,\n",
       " 'eval_steps_per_second': 59.327}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cd93fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "426.87255859375"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated() / (1024 ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502ca674",
   "metadata": {},
   "source": [
    "## 2. Full finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebc069ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9aa01da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_memory_and_metrics(trainer):\n",
    "    num_params = sum(p.numel() for p in trainer.model.parameters() if p.requires_grad)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    start = time.time()\n",
    "    trainer.train()\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    results = trainer.evaluate(dataset[\"test\"])\n",
    "\n",
    "    mem_mb = torch.cuda.max_memory_allocated() / (1024 ** 2)\n",
    "\n",
    "    print(f\"Количество обучаемых параметров: {num_params}\")\n",
    "    print(f\"Время: {elapsed:.2f} s\")\n",
    "    print(f\"Метрики: {results}\")\n",
    "    print(f\"Использование памяти gpu: {mem_mb:.2f} MB\")\n",
    "\n",
    "    return {\n",
    "        \"metrics\": results,\n",
    "        \"time_s\": elapsed,\n",
    "        \"mem_mb\": mem_mb,\n",
    "        \"num_params\": num_params\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d6d23e",
   "metadata": {},
   "source": [
    "Аргументы трейнера выбраны одинаковыми для всех видов дообучения для сравнения:\n",
    "- оптимизатор AdamW (по умолчанию в HF Trainer)\n",
    "- lr=2e-5 — стандарт для дообучения BERT-подобных\n",
    "- batch_size=16 — для укладывания в память\n",
    "- epochs=15 — даёт стабильную сходимость в данном случае"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1a9329",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./full_finetining\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=15,\n",
    "    weight_decay=0.01,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset['validation'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "818363a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15000' max='15000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15000/15000 24:13, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.228400</td>\n",
       "      <td>0.199299</td>\n",
       "      <td>0.929000</td>\n",
       "      <td>0.896605</td>\n",
       "      <td>0.917308</td>\n",
       "      <td>0.903498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.147000</td>\n",
       "      <td>0.158420</td>\n",
       "      <td>0.934500</td>\n",
       "      <td>0.927437</td>\n",
       "      <td>0.891392</td>\n",
       "      <td>0.906706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.110800</td>\n",
       "      <td>0.151150</td>\n",
       "      <td>0.942500</td>\n",
       "      <td>0.913778</td>\n",
       "      <td>0.926093</td>\n",
       "      <td>0.919284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.105100</td>\n",
       "      <td>0.195490</td>\n",
       "      <td>0.939500</td>\n",
       "      <td>0.921116</td>\n",
       "      <td>0.918304</td>\n",
       "      <td>0.918506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.067800</td>\n",
       "      <td>0.269534</td>\n",
       "      <td>0.934500</td>\n",
       "      <td>0.916425</td>\n",
       "      <td>0.907580</td>\n",
       "      <td>0.911728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.048900</td>\n",
       "      <td>0.312168</td>\n",
       "      <td>0.932500</td>\n",
       "      <td>0.911735</td>\n",
       "      <td>0.905060</td>\n",
       "      <td>0.908158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.036300</td>\n",
       "      <td>0.298440</td>\n",
       "      <td>0.940500</td>\n",
       "      <td>0.923355</td>\n",
       "      <td>0.914100</td>\n",
       "      <td>0.918547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.028900</td>\n",
       "      <td>0.396841</td>\n",
       "      <td>0.938500</td>\n",
       "      <td>0.922220</td>\n",
       "      <td>0.912749</td>\n",
       "      <td>0.917302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.012600</td>\n",
       "      <td>0.367114</td>\n",
       "      <td>0.939000</td>\n",
       "      <td>0.927469</td>\n",
       "      <td>0.910057</td>\n",
       "      <td>0.917900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.022400</td>\n",
       "      <td>0.381348</td>\n",
       "      <td>0.938500</td>\n",
       "      <td>0.924143</td>\n",
       "      <td>0.909212</td>\n",
       "      <td>0.915893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.017300</td>\n",
       "      <td>0.417529</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.924879</td>\n",
       "      <td>0.911672</td>\n",
       "      <td>0.917987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.015100</td>\n",
       "      <td>0.382510</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.925434</td>\n",
       "      <td>0.912594</td>\n",
       "      <td>0.918452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>0.386270</td>\n",
       "      <td>0.940500</td>\n",
       "      <td>0.920077</td>\n",
       "      <td>0.921656</td>\n",
       "      <td>0.920653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.399042</td>\n",
       "      <td>0.939000</td>\n",
       "      <td>0.916151</td>\n",
       "      <td>0.918493</td>\n",
       "      <td>0.917247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>0.407051</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.915953</td>\n",
       "      <td>0.915992</td>\n",
       "      <td>0.915873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество обучаемых параметров: 109486854\n",
      "Время: 1453.84 s\n",
      "Метрики: {'eval_loss': 0.4773561954498291, 'eval_accuracy': 0.9295, 'eval_precision': 0.8865127398653881, 'eval_recall': 0.885334840654208, 'eval_f1_macro': 0.8857581442879335, 'eval_runtime': 2.0, 'eval_samples_per_second': 999.985, 'eval_steps_per_second': 62.499, 'epoch': 15.0}\n",
      "Использование памяти gpu: 2153.35 MB\n"
     ]
    }
   ],
   "source": [
    "full_finetining_metrics = train_with_memory_and_metrics(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc356d3c",
   "metadata": {},
   "source": [
    "## 3. Linear probing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d4be83",
   "metadata": {},
   "source": [
    "LayerNorm → Linear → ReLU → Dropout → LayerNorm → Linear\n",
    "- Простая feed-forward сеть с двумя линейными проекциями, нормализацией и дропаутом, Dropout и LayerNorm помогают стабилизировать обучение и предотвратить переобучение в условиях малого числа обучаемых параметров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92c22ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearProbeHead(torch.nn.Module):\n",
    "    def __init__(self, hidden_size, num_labels):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.LayerNorm(hidden_size // 2),\n",
    "            nn.Linear(hidden_size // 2, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99a2a2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
    "model.classifier = LinearProbeHead(hidden_size=model.config.hidden_size, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1d9923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze BERT: обучаем только голову (~300k параметров) для быстрой отладки\n",
    "for name, param in model.named_parameters():\n",
    "    if not name.startswith(\"classifier\"):\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0af5b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./linear_probing\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=15,\n",
    "    weight_decay=0.01,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset['validation'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10e7c06a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15000' max='15000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15000/15000 05:59, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.572400</td>\n",
       "      <td>1.535964</td>\n",
       "      <td>0.427000</td>\n",
       "      <td>0.143386</td>\n",
       "      <td>0.227367</td>\n",
       "      <td>0.174668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.534000</td>\n",
       "      <td>1.481792</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.147962</td>\n",
       "      <td>0.235142</td>\n",
       "      <td>0.181445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.503500</td>\n",
       "      <td>1.454042</td>\n",
       "      <td>0.475500</td>\n",
       "      <td>0.156435</td>\n",
       "      <td>0.248939</td>\n",
       "      <td>0.192047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.478700</td>\n",
       "      <td>1.431136</td>\n",
       "      <td>0.480500</td>\n",
       "      <td>0.260956</td>\n",
       "      <td>0.256108</td>\n",
       "      <td>0.200059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.462000</td>\n",
       "      <td>1.403598</td>\n",
       "      <td>0.471000</td>\n",
       "      <td>0.287460</td>\n",
       "      <td>0.257093</td>\n",
       "      <td>0.207948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.450500</td>\n",
       "      <td>1.410270</td>\n",
       "      <td>0.486500</td>\n",
       "      <td>0.275933</td>\n",
       "      <td>0.260057</td>\n",
       "      <td>0.201708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.447000</td>\n",
       "      <td>1.384009</td>\n",
       "      <td>0.501500</td>\n",
       "      <td>0.315395</td>\n",
       "      <td>0.267122</td>\n",
       "      <td>0.213717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.429300</td>\n",
       "      <td>1.360353</td>\n",
       "      <td>0.499000</td>\n",
       "      <td>0.295384</td>\n",
       "      <td>0.267785</td>\n",
       "      <td>0.215956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.431800</td>\n",
       "      <td>1.360855</td>\n",
       "      <td>0.504000</td>\n",
       "      <td>0.507574</td>\n",
       "      <td>0.279196</td>\n",
       "      <td>0.237196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.431500</td>\n",
       "      <td>1.367918</td>\n",
       "      <td>0.498000</td>\n",
       "      <td>0.482270</td>\n",
       "      <td>0.273465</td>\n",
       "      <td>0.225655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.420600</td>\n",
       "      <td>1.348782</td>\n",
       "      <td>0.503500</td>\n",
       "      <td>0.472963</td>\n",
       "      <td>0.273777</td>\n",
       "      <td>0.229948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.414600</td>\n",
       "      <td>1.346895</td>\n",
       "      <td>0.504500</td>\n",
       "      <td>0.453538</td>\n",
       "      <td>0.274811</td>\n",
       "      <td>0.229517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.416400</td>\n",
       "      <td>1.348849</td>\n",
       "      <td>0.506500</td>\n",
       "      <td>0.451384</td>\n",
       "      <td>0.280339</td>\n",
       "      <td>0.238655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.406600</td>\n",
       "      <td>1.338499</td>\n",
       "      <td>0.512500</td>\n",
       "      <td>0.432038</td>\n",
       "      <td>0.284601</td>\n",
       "      <td>0.245103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.405900</td>\n",
       "      <td>1.342079</td>\n",
       "      <td>0.509500</td>\n",
       "      <td>0.420894</td>\n",
       "      <td>0.280415</td>\n",
       "      <td>0.237127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество обучаемых параметров: 299910\n",
      "Время: 359.18 s\n",
      "Метрики: {'eval_loss': 1.310072660446167, 'eval_accuracy': 0.503, 'eval_precision': 0.405377564086365, 'eval_recall': 0.2717470693308338, 'eval_f1_macro': 0.23209638260975293, 'eval_runtime': 2.0317, 'eval_samples_per_second': 984.413, 'eval_steps_per_second': 61.526, 'epoch': 15.0}\n",
      "Использование памяти gpu: 1274.20 MB\n"
     ]
    }
   ],
   "source": [
    "linear_probing_metrics = train_with_memory_and_metrics(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84fb043",
   "metadata": {},
   "source": [
    "## 4. Prefix tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f6fad0",
   "metadata": {},
   "source": [
    "Выбрала Prefix tuning:\n",
    "- Prefix Tuning создаёт виртуальные ключи (K) и значения (V) для механизма self-attention\n",
    "  на каждом слое. Это даёт более «глубокий» контроль над распределением внимания,\n",
    "  что особенно важно в задачах с тонкими контекстными различиями.\n",
    "  При этом основные веса модели остаются замороженными, а обучается лишь небольшой\n",
    "  объём префиксных параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9303b03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_config = PrefixTuningConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False,\n",
    "    num_virtual_tokens=20,\n",
    "    prefix_projection=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d7b353a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
    "model = get_peft_model(base_model, prefix_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "893fef6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 14,780,160 || all params: 124,267,014 || trainable%: 11.8939\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "736ee252",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./prefix_tuning\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=15,\n",
    "    weight_decay=0.01,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset['validation'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b6a32d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15000' max='15000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15000/15000 11:52, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.299300</td>\n",
       "      <td>1.189330</td>\n",
       "      <td>0.576500</td>\n",
       "      <td>0.197402</td>\n",
       "      <td>0.307434</td>\n",
       "      <td>0.237044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.119100</td>\n",
       "      <td>1.019686</td>\n",
       "      <td>0.653500</td>\n",
       "      <td>0.307044</td>\n",
       "      <td>0.395256</td>\n",
       "      <td>0.343401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.957200</td>\n",
       "      <td>0.857430</td>\n",
       "      <td>0.733000</td>\n",
       "      <td>0.455334</td>\n",
       "      <td>0.505883</td>\n",
       "      <td>0.467548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.818500</td>\n",
       "      <td>0.714534</td>\n",
       "      <td>0.799500</td>\n",
       "      <td>0.683223</td>\n",
       "      <td>0.598204</td>\n",
       "      <td>0.563844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.708900</td>\n",
       "      <td>0.585164</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.863506</td>\n",
       "      <td>0.776042</td>\n",
       "      <td>0.801802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.623200</td>\n",
       "      <td>0.516942</td>\n",
       "      <td>0.899000</td>\n",
       "      <td>0.874307</td>\n",
       "      <td>0.844231</td>\n",
       "      <td>0.853018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.573000</td>\n",
       "      <td>0.473891</td>\n",
       "      <td>0.913500</td>\n",
       "      <td>0.897855</td>\n",
       "      <td>0.871263</td>\n",
       "      <td>0.881269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.543200</td>\n",
       "      <td>0.447320</td>\n",
       "      <td>0.913500</td>\n",
       "      <td>0.877711</td>\n",
       "      <td>0.892905</td>\n",
       "      <td>0.884565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.513000</td>\n",
       "      <td>0.429057</td>\n",
       "      <td>0.919000</td>\n",
       "      <td>0.898433</td>\n",
       "      <td>0.883951</td>\n",
       "      <td>0.889593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.499400</td>\n",
       "      <td>0.419866</td>\n",
       "      <td>0.920500</td>\n",
       "      <td>0.899152</td>\n",
       "      <td>0.892237</td>\n",
       "      <td>0.892948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.481300</td>\n",
       "      <td>0.410849</td>\n",
       "      <td>0.920500</td>\n",
       "      <td>0.888780</td>\n",
       "      <td>0.896683</td>\n",
       "      <td>0.892234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.469500</td>\n",
       "      <td>0.397878</td>\n",
       "      <td>0.927000</td>\n",
       "      <td>0.907939</td>\n",
       "      <td>0.894090</td>\n",
       "      <td>0.899049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.474700</td>\n",
       "      <td>0.390720</td>\n",
       "      <td>0.926000</td>\n",
       "      <td>0.897760</td>\n",
       "      <td>0.896361</td>\n",
       "      <td>0.896631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.459100</td>\n",
       "      <td>0.385891</td>\n",
       "      <td>0.929000</td>\n",
       "      <td>0.901371</td>\n",
       "      <td>0.899157</td>\n",
       "      <td>0.899496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.453500</td>\n",
       "      <td>0.385549</td>\n",
       "      <td>0.927500</td>\n",
       "      <td>0.894464</td>\n",
       "      <td>0.896399</td>\n",
       "      <td>0.894887</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество обучаемых параметров: 14780160\n",
      "Время: 712.65 s\n",
      "Метрики: {'eval_loss': 0.39882901310920715, 'eval_accuracy': 0.9175, 'eval_precision': 0.8745803288345738, 'eval_recall': 0.8706279087832317, 'eval_f1_macro': 0.8714575933344596, 'eval_runtime': 2.2662, 'eval_samples_per_second': 882.53, 'eval_steps_per_second': 55.158, 'epoch': 15.0}\n",
      "Использование памяти gpu: 1231.53 MB\n"
     ]
    }
   ],
   "source": [
    "prefix_tuning_metrics = train_with_memory_and_metrics(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ec0f3e",
   "metadata": {},
   "source": [
    "## 5. LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ed7ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= LoRA: r=4 =========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15000' max='15000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15000/15000 10:42, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.508000</td>\n",
       "      <td>1.352425</td>\n",
       "      <td>0.527500</td>\n",
       "      <td>0.179383</td>\n",
       "      <td>0.281449</td>\n",
       "      <td>0.216507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.191800</td>\n",
       "      <td>1.128044</td>\n",
       "      <td>0.577000</td>\n",
       "      <td>0.363874</td>\n",
       "      <td>0.308370</td>\n",
       "      <td>0.238922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.076800</td>\n",
       "      <td>1.013305</td>\n",
       "      <td>0.603500</td>\n",
       "      <td>0.495377</td>\n",
       "      <td>0.352494</td>\n",
       "      <td>0.313696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.945300</td>\n",
       "      <td>0.892368</td>\n",
       "      <td>0.648500</td>\n",
       "      <td>0.708553</td>\n",
       "      <td>0.454681</td>\n",
       "      <td>0.424758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.861400</td>\n",
       "      <td>0.799344</td>\n",
       "      <td>0.682500</td>\n",
       "      <td>0.684464</td>\n",
       "      <td>0.500093</td>\n",
       "      <td>0.477441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.803900</td>\n",
       "      <td>0.735382</td>\n",
       "      <td>0.711500</td>\n",
       "      <td>0.737918</td>\n",
       "      <td>0.536221</td>\n",
       "      <td>0.544251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.742600</td>\n",
       "      <td>0.678475</td>\n",
       "      <td>0.743000</td>\n",
       "      <td>0.735738</td>\n",
       "      <td>0.592921</td>\n",
       "      <td>0.608466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.700200</td>\n",
       "      <td>0.638714</td>\n",
       "      <td>0.760500</td>\n",
       "      <td>0.732017</td>\n",
       "      <td>0.634975</td>\n",
       "      <td>0.653188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.677600</td>\n",
       "      <td>0.601813</td>\n",
       "      <td>0.778500</td>\n",
       "      <td>0.763902</td>\n",
       "      <td>0.658144</td>\n",
       "      <td>0.684174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.638400</td>\n",
       "      <td>0.579892</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.767874</td>\n",
       "      <td>0.677944</td>\n",
       "      <td>0.702255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.615600</td>\n",
       "      <td>0.554063</td>\n",
       "      <td>0.803500</td>\n",
       "      <td>0.778497</td>\n",
       "      <td>0.705619</td>\n",
       "      <td>0.729869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.597100</td>\n",
       "      <td>0.543356</td>\n",
       "      <td>0.802500</td>\n",
       "      <td>0.778847</td>\n",
       "      <td>0.698277</td>\n",
       "      <td>0.724001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.597900</td>\n",
       "      <td>0.529766</td>\n",
       "      <td>0.809500</td>\n",
       "      <td>0.790834</td>\n",
       "      <td>0.715135</td>\n",
       "      <td>0.740922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.581600</td>\n",
       "      <td>0.527558</td>\n",
       "      <td>0.811000</td>\n",
       "      <td>0.786388</td>\n",
       "      <td>0.718797</td>\n",
       "      <td>0.741923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.583500</td>\n",
       "      <td>0.523184</td>\n",
       "      <td>0.812000</td>\n",
       "      <td>0.786958</td>\n",
       "      <td>0.718703</td>\n",
       "      <td>0.742160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество обучаемых параметров: 152070\n",
      "Время: 642.32 s\n",
      "Метрики: {'eval_loss': 0.4835277795791626, 'eval_accuracy': 0.829, 'eval_precision': 0.7926230999087097, 'eval_recall': 0.7269297903006645, 'eval_f1_macro': 0.7512637486932152, 'eval_runtime': 2.1476, 'eval_samples_per_second': 931.271, 'eval_steps_per_second': 58.204, 'epoch': 15.0}\n",
      "Использование памяти gpu: 1624.60 MB\n",
      "========= LoRA: r=8 =========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15000' max='15000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15000/15000 10:42, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.320600</td>\n",
       "      <td>1.188959</td>\n",
       "      <td>0.565000</td>\n",
       "      <td>0.192644</td>\n",
       "      <td>0.301392</td>\n",
       "      <td>0.232054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.123500</td>\n",
       "      <td>1.067242</td>\n",
       "      <td>0.588000</td>\n",
       "      <td>0.202671</td>\n",
       "      <td>0.313873</td>\n",
       "      <td>0.242214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.955700</td>\n",
       "      <td>0.864970</td>\n",
       "      <td>0.659500</td>\n",
       "      <td>0.588888</td>\n",
       "      <td>0.455014</td>\n",
       "      <td>0.422883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.811700</td>\n",
       "      <td>0.771365</td>\n",
       "      <td>0.691000</td>\n",
       "      <td>0.666203</td>\n",
       "      <td>0.516473</td>\n",
       "      <td>0.491073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.738900</td>\n",
       "      <td>0.679381</td>\n",
       "      <td>0.734000</td>\n",
       "      <td>0.691089</td>\n",
       "      <td>0.572374</td>\n",
       "      <td>0.570135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.685000</td>\n",
       "      <td>0.614430</td>\n",
       "      <td>0.767500</td>\n",
       "      <td>0.773187</td>\n",
       "      <td>0.618862</td>\n",
       "      <td>0.643590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.615500</td>\n",
       "      <td>0.542913</td>\n",
       "      <td>0.798500</td>\n",
       "      <td>0.782376</td>\n",
       "      <td>0.679259</td>\n",
       "      <td>0.701688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.576100</td>\n",
       "      <td>0.495634</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.797809</td>\n",
       "      <td>0.744841</td>\n",
       "      <td>0.763528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.550800</td>\n",
       "      <td>0.461885</td>\n",
       "      <td>0.835000</td>\n",
       "      <td>0.811135</td>\n",
       "      <td>0.747983</td>\n",
       "      <td>0.770462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>0.436101</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.826677</td>\n",
       "      <td>0.783131</td>\n",
       "      <td>0.799150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.489500</td>\n",
       "      <td>0.412316</td>\n",
       "      <td>0.860500</td>\n",
       "      <td>0.837590</td>\n",
       "      <td>0.808317</td>\n",
       "      <td>0.820038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.464400</td>\n",
       "      <td>0.402301</td>\n",
       "      <td>0.860500</td>\n",
       "      <td>0.834992</td>\n",
       "      <td>0.801417</td>\n",
       "      <td>0.815286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.472000</td>\n",
       "      <td>0.390974</td>\n",
       "      <td>0.866500</td>\n",
       "      <td>0.841804</td>\n",
       "      <td>0.818218</td>\n",
       "      <td>0.827683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.447100</td>\n",
       "      <td>0.389029</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>0.843918</td>\n",
       "      <td>0.828581</td>\n",
       "      <td>0.834862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.454700</td>\n",
       "      <td>0.385041</td>\n",
       "      <td>0.870500</td>\n",
       "      <td>0.844776</td>\n",
       "      <td>0.828639</td>\n",
       "      <td>0.834993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество обучаемых параметров: 299526\n",
      "Время: 642.23 s\n",
      "Метрики: {'eval_loss': 0.3774574100971222, 'eval_accuracy': 0.8715, 'eval_precision': 0.8221311657385462, 'eval_recall': 0.8033526009307427, 'eval_f1_macro': 0.8111128190851408, 'eval_runtime': 2.1382, 'eval_samples_per_second': 935.379, 'eval_steps_per_second': 58.461, 'epoch': 15.0}\n",
      "Использование памяти gpu: 1628.76 MB\n",
      "========= LoRA: r=16 =========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15000' max='15000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15000/15000 10:43, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.221300</td>\n",
       "      <td>1.128124</td>\n",
       "      <td>0.583000</td>\n",
       "      <td>0.199886</td>\n",
       "      <td>0.311042</td>\n",
       "      <td>0.239805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.041200</td>\n",
       "      <td>0.927677</td>\n",
       "      <td>0.637500</td>\n",
       "      <td>0.443091</td>\n",
       "      <td>0.409069</td>\n",
       "      <td>0.383253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.800800</td>\n",
       "      <td>0.724905</td>\n",
       "      <td>0.732000</td>\n",
       "      <td>0.614309</td>\n",
       "      <td>0.560295</td>\n",
       "      <td>0.552399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.665500</td>\n",
       "      <td>0.601161</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.773729</td>\n",
       "      <td>0.631481</td>\n",
       "      <td>0.647141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.577600</td>\n",
       "      <td>0.500710</td>\n",
       "      <td>0.828000</td>\n",
       "      <td>0.816004</td>\n",
       "      <td>0.733133</td>\n",
       "      <td>0.757751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.509500</td>\n",
       "      <td>0.445828</td>\n",
       "      <td>0.843000</td>\n",
       "      <td>0.829358</td>\n",
       "      <td>0.746796</td>\n",
       "      <td>0.777206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.389147</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>0.844545</td>\n",
       "      <td>0.818995</td>\n",
       "      <td>0.829622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.433400</td>\n",
       "      <td>0.367373</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>0.853812</td>\n",
       "      <td>0.860372</td>\n",
       "      <td>0.855201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.391100</td>\n",
       "      <td>0.334151</td>\n",
       "      <td>0.889000</td>\n",
       "      <td>0.859636</td>\n",
       "      <td>0.850714</td>\n",
       "      <td>0.853827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.372200</td>\n",
       "      <td>0.313420</td>\n",
       "      <td>0.898500</td>\n",
       "      <td>0.867644</td>\n",
       "      <td>0.866968</td>\n",
       "      <td>0.867079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.348200</td>\n",
       "      <td>0.307244</td>\n",
       "      <td>0.898500</td>\n",
       "      <td>0.864889</td>\n",
       "      <td>0.872471</td>\n",
       "      <td>0.868523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.328400</td>\n",
       "      <td>0.299142</td>\n",
       "      <td>0.901500</td>\n",
       "      <td>0.872389</td>\n",
       "      <td>0.864698</td>\n",
       "      <td>0.867954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.341200</td>\n",
       "      <td>0.286966</td>\n",
       "      <td>0.906000</td>\n",
       "      <td>0.875973</td>\n",
       "      <td>0.879187</td>\n",
       "      <td>0.877277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.315100</td>\n",
       "      <td>0.288578</td>\n",
       "      <td>0.907000</td>\n",
       "      <td>0.871122</td>\n",
       "      <td>0.887031</td>\n",
       "      <td>0.878753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.320300</td>\n",
       "      <td>0.284464</td>\n",
       "      <td>0.909000</td>\n",
       "      <td>0.874393</td>\n",
       "      <td>0.887318</td>\n",
       "      <td>0.880559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество обучаемых параметров: 594438\n",
      "Время: 643.63 s\n",
      "Метрики: {'eval_loss': 0.2778737246990204, 'eval_accuracy': 0.9045, 'eval_precision': 0.8559403881670988, 'eval_recall': 0.8646255591482802, 'eval_f1_macro': 0.8595715039991082, 'eval_runtime': 2.1585, 'eval_samples_per_second': 926.58, 'eval_steps_per_second': 57.911, 'epoch': 15.0}\n",
      "Использование памяти gpu: 1636.13 MB\n",
      "========= LoRA: r=32 =========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15000' max='15000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15000/15000 10:47, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.161300</td>\n",
       "      <td>1.041651</td>\n",
       "      <td>0.603000</td>\n",
       "      <td>0.279227</td>\n",
       "      <td>0.338065</td>\n",
       "      <td>0.284661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.873700</td>\n",
       "      <td>0.759423</td>\n",
       "      <td>0.722000</td>\n",
       "      <td>0.690683</td>\n",
       "      <td>0.523063</td>\n",
       "      <td>0.525195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.668100</td>\n",
       "      <td>0.570412</td>\n",
       "      <td>0.810500</td>\n",
       "      <td>0.802108</td>\n",
       "      <td>0.695799</td>\n",
       "      <td>0.712243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.553500</td>\n",
       "      <td>0.476883</td>\n",
       "      <td>0.845500</td>\n",
       "      <td>0.843893</td>\n",
       "      <td>0.733451</td>\n",
       "      <td>0.758296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.469300</td>\n",
       "      <td>0.388643</td>\n",
       "      <td>0.879500</td>\n",
       "      <td>0.856225</td>\n",
       "      <td>0.821467</td>\n",
       "      <td>0.836462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.395000</td>\n",
       "      <td>0.327499</td>\n",
       "      <td>0.906000</td>\n",
       "      <td>0.880971</td>\n",
       "      <td>0.859342</td>\n",
       "      <td>0.865470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.344100</td>\n",
       "      <td>0.300885</td>\n",
       "      <td>0.910500</td>\n",
       "      <td>0.895904</td>\n",
       "      <td>0.870264</td>\n",
       "      <td>0.881512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>0.280700</td>\n",
       "      <td>0.916000</td>\n",
       "      <td>0.884613</td>\n",
       "      <td>0.894377</td>\n",
       "      <td>0.889169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.289900</td>\n",
       "      <td>0.266553</td>\n",
       "      <td>0.920500</td>\n",
       "      <td>0.893514</td>\n",
       "      <td>0.890402</td>\n",
       "      <td>0.891752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.280500</td>\n",
       "      <td>0.256258</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.889338</td>\n",
       "      <td>0.895613</td>\n",
       "      <td>0.891733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.264200</td>\n",
       "      <td>0.248145</td>\n",
       "      <td>0.921000</td>\n",
       "      <td>0.887793</td>\n",
       "      <td>0.900506</td>\n",
       "      <td>0.893374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.235900</td>\n",
       "      <td>0.231605</td>\n",
       "      <td>0.923500</td>\n",
       "      <td>0.900317</td>\n",
       "      <td>0.889597</td>\n",
       "      <td>0.894588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.268300</td>\n",
       "      <td>0.231923</td>\n",
       "      <td>0.929500</td>\n",
       "      <td>0.900365</td>\n",
       "      <td>0.906428</td>\n",
       "      <td>0.903133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.223300</td>\n",
       "      <td>0.235312</td>\n",
       "      <td>0.926000</td>\n",
       "      <td>0.893919</td>\n",
       "      <td>0.905967</td>\n",
       "      <td>0.899668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.233300</td>\n",
       "      <td>0.232021</td>\n",
       "      <td>0.927000</td>\n",
       "      <td>0.897239</td>\n",
       "      <td>0.908747</td>\n",
       "      <td>0.902688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество обучаемых параметров: 1184262\n",
      "Время: 647.50 s\n",
      "Метрики: {'eval_loss': 0.24380642175674438, 'eval_accuracy': 0.914, 'eval_precision': 0.8627140297875124, 'eval_recall': 0.8685216879504312, 'eval_f1_macro': 0.865518094629688, 'eval_runtime': 2.1585, 'eval_samples_per_second': 926.556, 'eval_steps_per_second': 57.91, 'epoch': 15.0}\n",
      "Использование памяти gpu: 1642.61 MB\n"
     ]
    }
   ],
   "source": [
    "r_values = [4, 8, 16, 32]\n",
    "lora_experiments = []\n",
    "\n",
    "for r in r_values:\n",
    "    print(f\"========= LoRA: r={r} =========\")\n",
    "    # 1) Базовая модель\n",
    "    base = AutoModelForSequenceClassification.from_pretrained(model_checkpoint,num_labels=num_labels)\n",
    "\n",
    "    lora_cfg = LoraConfig(\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        inference_mode=False,\n",
    "        r=r,\n",
    "        lora_alpha=r * 4,      # часто alpha = r*X, где X из [1..4], чтобы было масштабирование адаптеров\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=[\"query\", \"value\"]\n",
    "    )\n",
    "\n",
    "    lora_model = get_peft_model(base, lora_cfg)\n",
    "    lora_model.to(device)\n",
    "\n",
    "    # 3) Trainer для LoRA\n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"./lora_r{r}\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=15,\n",
    "        weight_decay=0.01,\n",
    "        seed=SEED,)\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=lora_model,\n",
    "        args=args,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"validation\"],\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # 4) Запуск обучения + оценки\n",
    "    res = train_with_memory_and_metrics(trainer)\n",
    "    res[\"r\"] = r\n",
    "    lora_experiments.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0eb0b6be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r</th>\n",
       "      <th>trainable_params_K</th>\n",
       "      <th>time_s</th>\n",
       "      <th>mem_MB</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>152.070</td>\n",
       "      <td>642.315216</td>\n",
       "      <td>1624.602051</td>\n",
       "      <td>0.8290</td>\n",
       "      <td>0.792623</td>\n",
       "      <td>0.726930</td>\n",
       "      <td>0.751264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>299.526</td>\n",
       "      <td>642.233332</td>\n",
       "      <td>1628.762207</td>\n",
       "      <td>0.8715</td>\n",
       "      <td>0.822131</td>\n",
       "      <td>0.803353</td>\n",
       "      <td>0.811113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>594.438</td>\n",
       "      <td>643.630512</td>\n",
       "      <td>1636.133301</td>\n",
       "      <td>0.9045</td>\n",
       "      <td>0.855940</td>\n",
       "      <td>0.864626</td>\n",
       "      <td>0.859572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>1184.262</td>\n",
       "      <td>647.497522</td>\n",
       "      <td>1642.613770</td>\n",
       "      <td>0.9140</td>\n",
       "      <td>0.862714</td>\n",
       "      <td>0.868522</td>\n",
       "      <td>0.865518</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    r  trainable_params_K      time_s       mem_MB  accuracy  precision  \\\n",
       "0   4             152.070  642.315216  1624.602051    0.8290   0.792623   \n",
       "1   8             299.526  642.233332  1628.762207    0.8715   0.822131   \n",
       "2  16             594.438  643.630512  1636.133301    0.9045   0.855940   \n",
       "3  32            1184.262  647.497522  1642.613770    0.9140   0.862714   \n",
       "\n",
       "     recall  f1_macro  \n",
       "0  0.726930  0.751264  \n",
       "1  0.803353  0.811113  \n",
       "2  0.864626  0.859572  \n",
       "3  0.868522  0.865518  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lora = pd.DataFrame([{\n",
    "    \"r\": e[\"r\"],\n",
    "    \"trainable_params_K\": e[\"num_params\"] / 1e3,\n",
    "    \"time_s\": e[\"time_s\"],\n",
    "    \"mem_MB\": e[\"mem_mb\"],\n",
    "    \"accuracy\": e[\"metrics\"][\"eval_accuracy\"],\n",
    "    \"precision\": e[\"metrics\"][\"eval_precision\"],\n",
    "    \"recall\": e[\"metrics\"][\"eval_recall\"],\n",
    "    \"f1_macro\": e[\"metrics\"][\"eval_f1_macro\"]\n",
    "} for e in lora_experiments])\n",
    "df_lora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9177f9e",
   "metadata": {},
   "source": [
    "Подбирала ранги r {4,8,16,32}, лучший компромисс r=32, \n",
    "- несмотря на рост обучаемых параметров, время дообучения и занимаемая память не сильно растут в значениях по мере роста r, но метрики становятся значительно лучше"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698f6d16",
   "metadata": {},
   "source": [
    "## 6. Сравнение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6d1be9",
   "metadata": {},
   "source": [
    "| Метод               | Параметров   | Память (MB) | Время (s) | Accuracy | Precision | Recall  | F1      |\n",
    "|---------------------|--------------|-------------|-----------|----------|-----------|---------|---------|\n",
    "| До finetuning       | n/a          | n/a         | n/a       | 0.151    | 0.1449    | 0.1619  | 0.0729  |\n",
    "| Full finetuning     | 109 486 854  | 2153.35     | 1453.84   | 0.9295   | 0.8865    | 0.8853  | 0.8858  |\n",
    "| Linear probing      |   299 910    | 1274.20     | 359.18    | 0.5030   | 0.4054    | 0.2717  | 0.2321  |\n",
    "| Prefix tuning       | 14 780 160   | 1231.53     | 712.65    | 0.9175   | 0.8746    | 0.8706  | 0.8715  |\n",
    "| LoRA (r=32)         | 1 184 262    | 1642.61     | 647.50    | 0.9140   | 0.8627    | 0.8685  | 0.8655  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd3dfe6",
   "metadata": {},
   "source": [
    "- Full FT даёт максимальное качество (F1 0.886, accuracy 0.929), но требует около 2 GB памяти и наибольшее количество времени 1450 s\n",
    "- Prefix Tuning почти не уступает (F1 0.871) при halved памяти (1.2 GB) и времени (713 s).\n",
    "- LoRA (r=32) обучает всего ~1 M параметров, сохраняя высокое качество (F1≈0.866) и средние ресурсы (~1.6 GB, 647 s).\n",
    "- Linear Probing очень легковесен (300 k параметров, 359 s), но качество довольно низкое (F1 0.23)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
