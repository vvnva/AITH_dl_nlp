{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "763aaa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import warnings\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymorphy3\n",
    "from tqdm import tqdm\n",
    "from corus import load_lenta\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "import hdbscan\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "tqdm.pandas()\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "random.seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f908d2",
   "metadata": {},
   "source": [
    "## 1. Загрузка и предобработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8e79d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl -L https://github.com/yutkin/Lenta.Ru-News-Dataset/releases/download/v1.0/lenta-ru-news.csv.gz -o lenta-ru-news.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7463c904",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = load_lenta('lenta-ru-news.csv.gz')\n",
    "data = pd.DataFrame(records)\n",
    "data.columns = ['url','title','text','topic','tags','date']\n",
    "data = data[['title','text']].sample(n=50000, random_state=RANDOM_STATE)\n",
    "texts = (data['title'] + ' ' + data['text']).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c58c250",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\verai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 2. Предобработка текстов\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "morph = pymorphy3.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e39bcc",
   "metadata": {},
   "source": [
    "Для тематического моделирования ключевыми являются лемматизация и удаление шумовых токенов (запятых, цифр, стоп-слов), чтобы алгоритм не выделял темы по служебным словам. также возможна необходима фильтрация по длине документа, приведение к нижнему регистру, удаление HTML-артефактов, эмодзи и т. п.\n",
    "\n",
    "Реализуем функцию предобработки текстов. В данной функции:\n",
    "- Приводим текст к нижнему регистру.\n",
    "- Удаляем HTML-теги.\n",
    "- Оставляем только символы русского и английского алфавитов и пробелы.\n",
    "- Производим токенизацию и удаляем стоп-слова.\n",
    "- Выполняем лемматизацию с помощью pymorphy3.\n",
    "\n",
    "Базовая классическая предобработка, помогает унифицировать представление текста, снизить размерность пространства признаков за счет сведения различных форм слова к единой норме и уменьшить шум. Остановилась на ней, так как лемматизация может давать потенциально лучше результат, остальные методы довольно базовые, не занимают много времени на обработку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "301b7bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    # Нижний регистр\n",
    "    text = text.lower()\n",
    "    # Удаление HTML-тегов\n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "    # Удаление всех символов, кроме букв и пробелов\n",
    "    text = re.sub(r'[^a-zа-яё\\s]', ' ', text)\n",
    "    # Токенизация и удаление стоп-слов\n",
    "    tokens = [w for w in text.split() if w not in stop_words]\n",
    "    # Лемматизация\n",
    "    lemmas = [morph.parse(w)[0].normal_form for w in tokens]\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "444c7bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|██████████| 50000/50000 [07:06<00:00, 117.13it/s]\n"
     ]
    }
   ],
   "source": [
    "clean_texts = [preprocess(doc) for doc in tqdm(texts, desc='Preprocessing')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6828a26",
   "metadata": {},
   "source": [
    "## 2. Определяем компоненты пайплайна и обучение BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe4e6f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Encoder: sentence-transformers\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # Быстрая модель с хорошим балансом качества и скорости"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c7f534",
   "metadata": {},
   "source": [
    "Обычно UMAP (или t-SNE, PCA) для понижения размерности эмбеддингов перед кластеризацией. UMAP хорошо сохраняет глобальную и локальную структуру, важны гиперпараметры n_neighbors, min_dist для контроля расстояний между документами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e82d1ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP для снижения размерности перед кластеризацией\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=15,      # баланс локального/глобального\n",
    "    n_components=5,       # небольшой размер для кластеров, позже можно визуализировать через 2D-преобразование, хватит для сохранении информации\n",
    "    min_dist=0.1,         # плотное размещение точек, но тк тексты разные возьму больше 0\n",
    "    metric=\"cosine\", \n",
    "    random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4d5e6b",
   "metadata": {},
   "source": [
    "HDBSCAN — иерархический плотностный алгоритм, умеющий выделять документы без явной тематики, +не требует количества кластеров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f069773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDBSCAN для кластеризации\n",
    "hdbscan_model = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=50,      # минимальный размер темы, не берем слишком маленькие группы\n",
    "    metric='euclidean',       # расстояние в embedding-пространстве\n",
    "    prediction_data=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a397dbb1",
   "metadata": {},
   "source": [
    "CountVectorizer:\n",
    "- ngram_range=(1,2): учитывает униграммы и биграммы,\n",
    "- min_df=5: исключает слова, встречающиеся менее чем в 5 документах,\n",
    "- max_df=0.9: исключает слишком частые слова (более чем в 90% документов).\n",
    "\n",
    "Позволяет улучшить качество тем, отсекая редкие шумовые и слишком общие токены."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54eb273f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_model = CountVectorizer(ngram_range=(1, 2), min_df=5, max_df=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cb0571",
   "metadata": {},
   "source": [
    "ClassTFIDFTransformer:\n",
    "- reduce_frequent_words=True: дополнительно снижает вес часто встречающихся токенов,\n",
    "Для извлечения более информативных топ-токенов для каждой темы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9657ed73",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3fd81e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация и обучение BERTopic\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    ctfidf_model=ctfidf_model,\n",
    "    language=\"russian\",\n",
    "    calculate_probabilities=True, \n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d77879d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 12:02:09,364 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6d8a2d4c8884c3e9435a0bd0789fba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 12:03:02,831 - BERTopic - Embedding - Completed ✓\n",
      "2025-05-01 12:03:02,832 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-05-01 12:03:46,266 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-05-01 12:03:46,267 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-05-01 12:03:52,502 - BERTopic - Cluster - Completed ✓\n",
      "2025-05-01 12:03:52,509 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-05-01 12:04:02,162 - BERTopic - Representation - Completed ✓\n"
     ]
    }
   ],
   "source": [
    "topics, probabilities = topic_model.fit_transform(clean_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d79d1d",
   "metadata": {},
   "source": [
    "## 3. Визуализация результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f16de6ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>10079</td>\n",
       "      <td>-1_самолёт_учёный_матч_рубль</td>\n",
       "      <td>[самолёт, учёный, матч, рубль, погибнуть, of, ...</td>\n",
       "      <td>[судья бальтасар гарсон завести дело отношение...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>21191</td>\n",
       "      <td>0_украина_рубль_риа_риа новость</td>\n",
       "      <td>[украина, рубль, риа, риа новость, задержать, ...</td>\n",
       "      <td>[москва рассказать желание запад напугать евро...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3193</td>\n",
       "      <td>1_цска_цб_церковь_банк</td>\n",
       "      <td>[цска, цб, церковь, банк, цру, центробанк, цик...</td>\n",
       "      <td>[цска зенит сыграть вничью забить шесть мяч че...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2818</td>\n",
       "      <td>2_школа_шахта_школьник_шарапов</td>\n",
       "      <td>[школа, шахта, школьник, шарапов, ученик, ребё...</td>\n",
       "      <td>[шестилетний россиянин победить шоу талант кит...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>2579</td>\n",
       "      <td>3_мэр_аэропорт_венесуэла_аэрофлот</td>\n",
       "      <td>[мэр, аэропорт, венесуэла, аэрофлот, аэс, рао,...</td>\n",
       "      <td>[росэнергоатом ростелеком создать крупный дата...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                               Name  \\\n",
       "0     -1  10079       -1_самолёт_учёный_матч_рубль   \n",
       "1      0  21191    0_украина_рубль_риа_риа новость   \n",
       "2      1   3193             1_цска_цб_церковь_банк   \n",
       "3      2   2818     2_школа_шахта_школьник_шарапов   \n",
       "4      3   2579  3_мэр_аэропорт_венесуэла_аэрофлот   \n",
       "\n",
       "                                      Representation  \\\n",
       "0  [самолёт, учёный, матч, рубль, погибнуть, of, ...   \n",
       "1  [украина, рубль, риа, риа новость, задержать, ...   \n",
       "2  [цска, цб, церковь, банк, цру, центробанк, цик...   \n",
       "3  [школа, шахта, школьник, шарапов, ученик, ребё...   \n",
       "4  [мэр, аэропорт, венесуэла, аэрофлот, аэс, рао,...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [судья бальтасар гарсон завести дело отношение...  \n",
       "1  [москва рассказать желание запад напугать евро...  \n",
       "2  [цска зенит сыграть вничью забить шесть мяч че...  \n",
       "3  [шестилетний россиянин победить шоу талант кит...  \n",
       "4  [росэнергоатом ростелеком создать крупный дата...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Топ-токены для каждой темы\n",
    "topic_info = topic_model.get_topic_info()\n",
    "topic_info.head()  # наиболее важные темы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19b82d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#количество тем\n",
    "len(topic_model.get_topics()) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4787015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Топ-токены темы 1: [('украина', 0.12736140823997968), ('рубль', 0.12617583570491211), ('риа', 0.12009068702217965), ('риа новость', 0.1199587344119356), ('задержать', 0.11701731924845428), ('уголовный', 0.11480504590794754), ('путин', 0.11469298130600754), ('депутат', 0.11271316563716445), ('республика', 0.10988583316732341), ('орган', 0.10937164885992962)]\n"
     ]
    }
   ],
   "source": [
    "# Список топ токенов для темы #0\n",
    "print(\"Топ-токены темы 1:\", topic_model.get_topic(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17581ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D визуализация документов\n",
    "fig1 = topic_model.visualize_documents(clean_texts)\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff6ea4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Распределение тем по токенам для заданного документа\n",
    "doc_id = 17\n",
    "print(clean_texts[doc_id])\n",
    "fig_dist = topic_model.visualize_distribution(probabilities[doc_id], min_probability=0.015)\n",
    "fig_dist.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b2aed1",
   "metadata": {},
   "source": [
    "## 4. Оценка качества тем"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda83c84",
   "metadata": {},
   "source": [
    "### 4.1 Topic Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7eec919e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_diversity(model, top_n=10):\n",
    "    '''Доля уникальных токенов среди всех топ-N токенов.'''\n",
    "    topics = model.get_topics()\n",
    "    all_tokens = []\n",
    "    for _, tokens in topics.items():\n",
    "        all_tokens.extend([t for t, _ in tokens[:top_n]])\n",
    "    unique_count = len(set(all_tokens))\n",
    "    total_count = len(all_tokens)\n",
    "    return unique_count / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "694df111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic_diversity: 0.9142857142857143\n"
     ]
    }
   ],
   "source": [
    "td = topic_diversity(topic_model, top_n=10)\n",
    "print('topic_diversity:', td)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328b7e86",
   "metadata": {},
   "source": [
    "### 4.2 UMass Coherence через Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "afd7b998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для этого необходимо подготовить корпус токенов\n",
    "tokenized_texts = [doc.split() for doc in clean_texts]\n",
    "\n",
    "dictionary = Dictionary(tokenized_texts)\n",
    "corpus_gensim = [dictionary.doc2bow(text) for text in tokenized_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ab3b8ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_umass_coherence(model, texts, dictionary, corpus, top_n=10):\n",
    "    '''Вычисляет UMass coherence для каждой темы и среднее значение.'''\n",
    "    topics = [ [w for w, _ in model.get_topic(t)[:top_n]]\n",
    "               for t in model.get_topic_info().Topic if t != -1 ]\n",
    "    cm = CoherenceModel(\n",
    "        topics=topics,\n",
    "        texts=texts,\n",
    "        dictionary=dictionary,\n",
    "        corpus=corpus,\n",
    "        coherence='u_mass'\n",
    "    )\n",
    "    return cm.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ea6cea05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMass Coherence: -4.5876\n"
     ]
    }
   ],
   "source": [
    "umass_score = compute_umass_coherence(topic_model, tokenized_texts, dictionary, corpus_gensim)\n",
    "print(f\"UMass Coherence: {umass_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f008a7b",
   "metadata": {},
   "source": [
    "### Выводы\n",
    "- **Topic Diversity ≈ 0.91**  \n",
    "  Темы получились разнородными — 90 % токенов в топ-10 уникальны.\n",
    "\n",
    "- **UMass Coherence ≈ –4.6**  \n",
    "  Стабильный, но средний уровень когерентности; есть «размытые» темы.\n",
    "\n",
    "---\n",
    "\n",
    "Что можно исправить\n",
    "\n",
    "1. Поэкспериментировать с HDBSCAN  \n",
    "   – увеличить cluster_selection_epsilon для слияния близких тем  \n",
    "   – скорректировать min_samples для жёсткой фильтрации шума  \n",
    "2. добавить биграммы/триграммы для векторизации (ngram_range=(1,3)) или POS-фильтрацию  \n",
    "3. Эмбеддинги– протестировать другие Эмбеддинги, например RuBERT-модели \n",
    "4. UMAP – увеличить n_components (10–15) перед кластеризацией"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
