{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe6a37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Библиотеки и настройки\n",
    "import gc\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import evaluate\n",
    "from seqeval.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "from corus import load_ne5\n",
    "import corus.sources.ne5 as ne5\n",
    "import razdel\n",
    "from razdel.substring import Substring\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff70d6d",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60508e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl http://www.labinform.ru/pub/named_entities/collection5.zip -L -o data/collection5.zip\n",
    "# \n",
    "# with zipfile.ZipFile(\"data/collection5.zip\", 'r') as z:\n",
    "#     z.extractall(\"data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3cba8b",
   "metadata": {},
   "source": [
    "Загрузка и парсинг аннотированного корпуса NE5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b138e7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_utf8(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "ne5.load_text = load_text_utf8\n",
    "corpus = list(load_ne5(\"data/Collection5/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b9aea79",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Ne5Span:\n",
    "    start: int\n",
    "    stop: int\n",
    "    type: str\n",
    "\n",
    "@dataclass\n",
    "class LabeledText:\n",
    "    text: str\n",
    "    entities: list[Ne5Span]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be1e4e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ds_ner_fixed(corpus: list) -> tuple[list[LabeledText], dict[str, int]]:\n",
    "    labeled_texts = []\n",
    "    cats_map = {}\n",
    "    for annot in corpus:\n",
    "        entities = []\n",
    "        for ent in annot.spans:\n",
    "            new_ent = Ne5Span(\n",
    "                start=ent.start,\n",
    "                stop=ent.stop,\n",
    "                type=ent.type\n",
    "            )\n",
    "            entities.append(new_ent)\n",
    "            if new_ent.type not in cats_map:\n",
    "                cats_map[new_ent.type] = len(cats_map)\n",
    "        entities.sort(key=lambda x: x.start)\n",
    "        lab_txt = LabeledText(\n",
    "            text=annot.text,\n",
    "            entities=entities\n",
    "        )\n",
    "        labeled_texts.append(lab_txt)\n",
    "    return labeled_texts, cats_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0edab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Сплит train/test\n",
    "def train_test_split(data: list[LabeledText], test_n: int, random_seed: int = 0) -> tuple[list[LabeledText], list[LabeledText]]:\n",
    "    total_n = len(data)\n",
    "    print(f'train data: {total_n - test_n}')\n",
    "    print(f'test data: {test_n}')\n",
    "    data_idxs = range(total_n)\n",
    "    random.seed(random_seed)\n",
    "    test_idxs = sorted(random.sample(data_idxs, test_n))\n",
    "    train_idxs = sorted(set(data_idxs) - set(test_idxs))\n",
    "    data_test = [data[test_img_idx] for test_img_idx in test_idxs]\n",
    "    data_train = [data[train_img_idx] for train_img_idx in train_idxs]\n",
    "    return data_train, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ea98484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texts_n: 1000\n",
      "{'GEOPOLIT': 0, 'LOC': 1, 'MEDIA': 2, 'PER': 3, 'ORG': 4}\n"
     ]
    }
   ],
   "source": [
    "labeled_text, categories_map = parse_ds_ner_fixed(corpus)\n",
    "\n",
    "print(f'texts_n: {len(labeled_text)}')\n",
    "print(categories_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a16d4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LabeledText(text='Россия рассчитывает на конструктивное воздействие США на Грузию\\n\\n04/08/2008 12:08\\n\\nМОСКВА, 4 авг - РИА Новости. Россия рассчитывает, что США воздействуют на Тбилиси в связи с обострением ситуации в зоне грузино-осетинского конфликта. Об этом статс-секретарь - заместитель министра иностранных дел России Григорий Карасин заявил в телефонном разговоре с заместителем госсекретаря США Дэниэлом Фридом.\\n\\n\"С российской стороны выражена глубокая озабоченность в связи с новым витком напряженности вокруг Южной Осетии, противозаконными действиями грузинской стороны по наращиванию своих вооруженных сил в регионе, бесконтрольным строительством фортификационных сооружений\", - говорится в сообщении.\\n\\n\"Россия уже призвала Тбилиси к ответственной линии и рассчитывает также на конструктивное воздействие со стороны Вашингтона\", - сообщил МИД России. ', entities=[Ne5Span(start=0, stop=6, type='GEOPOLIT'), Ne5Span(start=50, stop=53, type='GEOPOLIT'), Ne5Span(start=57, stop=63, type='GEOPOLIT'), Ne5Span(start=87, stop=93, type='LOC'), Ne5Span(start=103, stop=114, type='MEDIA'), Ne5Span(start=116, stop=122, type='GEOPOLIT'), Ne5Span(start=141, stop=144, type='GEOPOLIT'), Ne5Span(start=161, stop=168, type='GEOPOLIT'), Ne5Span(start=301, stop=307, type='GEOPOLIT'), Ne5Span(start=308, stop=324, type='PER'), Ne5Span(start=383, stop=386, type='GEOPOLIT'), Ne5Span(start=387, stop=402, type='PER'), Ne5Span(start=505, stop=517, type='GEOPOLIT'), Ne5Span(start=703, stop=709, type='GEOPOLIT'), Ne5Span(start=723, stop=730, type='GEOPOLIT'), Ne5Span(start=815, stop=825, type='GEOPOLIT'), Ne5Span(start=838, stop=841, type='ORG'), Ne5Span(start=842, stop=848, type='GEOPOLIT')])\n"
     ]
    }
   ],
   "source": [
    "print(labeled_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebd88ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: 800\n",
      "test data: 200\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(\n",
    "    labeled_text,\n",
    "    test_n=200,\n",
    "    random_seed = random_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cdc650",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b012b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLOR_MAP = {\n",
    "    'GEOPOLIT': 'yellow',\n",
    "    'LOC': 'lightblue',\n",
    "    'MEDIA': \"lightgreen\",\n",
    "    'PER': \"pink\",\n",
    "    'ORG': \"lightgrey\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c72d472b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_labeled_text(labeled_text: LabeledText, color_map: dict[str, str]):\n",
    "    \"\"\"\n",
    "    Visualize the labeled data for Named Entity Recognition (NER).\n",
    "\n",
    "    Args:\n",
    "        labeled_text: An instance of LabeledText containing the text and entities.\n",
    "        color_map: A color map for each entity category.\n",
    "\n",
    "    Returns:\n",
    "        None. Displays the HTML representation of the text with highlighted entities.\n",
    "    \"\"\"\n",
    "    # Initialize the HTML string\n",
    "    labeled_text2 = labeled_text\n",
    "    labeled_text2.text = labeled_text.text.replace(\"\\n\", \"  \")\n",
    "    html_str = \"\"\n",
    "    last_index = 0\n",
    "    # Sort entities by the starting index\n",
    "    entities = sorted(labeled_text2.entities, key=lambda x: x.start)\n",
    "    for entity in entities:\n",
    "        start, stop, type = entity.start, entity.stop, entity.type\n",
    "        html_str += labeled_text2.text[last_index:start]  # non-entity text before the current entity\n",
    "        color = color_map[type]\n",
    "        html_str += (   # entity text with the corresponding color\n",
    "            f'<mark style=\"background-color: {color}\">{labeled_text2.text[start:stop]}'\n",
    "            + f'<sub>({type})</sub></mark>'\n",
    "        )\n",
    "        last_index = stop\n",
    "    html_str += labeled_text2.text[last_index:]  # remaining text after the last entity\n",
    "    display(HTML(html_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "131d8df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LabeledText(text='Барак Обама назначил губернатора Юты послом США в Китае\\nПрезидент США Барак Обама 16 мая назначил губернатора штата Юта Джона Хантсмена-младшего (John Huntsman Jr.) послом США в Китае, пишет The Washington Times.\\n\\nПо словам представителя Белого дома, кандидатуру Хантсмена Обаме предложил помощник по азиатской политике Джефф Бэйдер (Jeff Bader), который представил губернатора Юты как человека, отлично знающего китайский язык, разбирающегося в проблемах региона и способного эффективно решать дипломатические задачи.\\n\\nЧлены Республиканской партии, в том числе и бывший соперник Обамы на выборах президента США Джон Маккейн, поприветствовали выбор Обамы. Они, однако, в то же время признали, что в связи с этим назначением Хантсмен фактически лишился возможности участвовать в следующих президентских выборах.\\n\\nВо время предвыборной кампании 2008 года Хантсмен был одним из руководителей штаба Джона Маккейна и, по оценкам экспертов, именно он мог стать кандидатом от республиканцев на следующих выборах. Многие республиканцы отметили, что, сделав Хантсмена послом, Обама устранил потенциально опасного соперника.\\n\\nДо избрания губернатором Юты в 2004 году Хантсмен работал торговым представителем США при Джордже Буше-младшем, а до этого был послом США в Сингапуре. ', entities=[Ne5Span(start=0, stop=11, type='PER'), Ne5Span(start=33, stop=36, type='LOC'), Ne5Span(start=44, stop=47, type='GEOPOLIT'), Ne5Span(start=50, stop=55, type='GEOPOLIT'), Ne5Span(start=67, stop=70, type='GEOPOLIT'), Ne5Span(start=71, stop=82, type='PER'), Ne5Span(start=117, stop=120, type='LOC'), Ne5Span(start=121, stop=165, type='PER'), Ne5Span(start=173, stop=176, type='GEOPOLIT'), Ne5Span(start=179, stop=184, type='GEOPOLIT'), Ne5Span(start=192, stop=212, type='MEDIA'), Ne5Span(start=241, stop=252, type='GEOPOLIT'), Ne5Span(start=266, stop=275, type='PER'), Ne5Span(start=276, stop=281, type='PER'), Ne5Span(start=323, stop=348, type='PER'), Ne5Span(start=381, stop=384, type='LOC'), Ne5Span(start=531, stop=553, type='ORG'), Ne5Span(start=585, stop=590, type='PER'), Ne5Span(start=613, stop=616, type='GEOPOLIT'), Ne5Span(start=617, stop=629, type='PER'), Ne5Span(start=654, stop=659, type='PER'), Ne5Span(start=729, stop=737, type='PER'), Ne5Span(start=860, stop=868, type='PER'), Ne5Span(start=902, stop=916, type='PER'), Ne5Span(start=1056, stop=1065, type='PER'), Ne5Span(start=1074, stop=1079, type='PER'), Ne5Span(start=1150, stop=1153, type='LOC'), Ne5Span(start=1166, stop=1174, type='PER'), Ne5Span(start=1207, stop=1210, type='GEOPOLIT'), Ne5Span(start=1215, stop=1235, type='PER'), Ne5Span(start=1259, stop=1262, type='GEOPOLIT'), Ne5Span(start=1265, stop=1274, type='GEOPOLIT')]) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<mark style=\"background-color: pink\">Барак Обама<sub>(PER)</sub></mark> назначил губернатора <mark style=\"background-color: lightblue\">Юты<sub>(LOC)</sub></mark> послом <mark style=\"background-color: yellow\">США<sub>(GEOPOLIT)</sub></mark> в <mark style=\"background-color: yellow\">Китае<sub>(GEOPOLIT)</sub></mark>  Президент <mark style=\"background-color: yellow\">США<sub>(GEOPOLIT)</sub></mark> <mark style=\"background-color: pink\">Барак Обама<sub>(PER)</sub></mark> 16 мая назначил губернатора штата <mark style=\"background-color: lightblue\">Юта<sub>(LOC)</sub></mark> <mark style=\"background-color: pink\">Джона Хантсмена-младшего (John Huntsman Jr.)<sub>(PER)</sub></mark> послом <mark style=\"background-color: yellow\">США<sub>(GEOPOLIT)</sub></mark> в <mark style=\"background-color: yellow\">Китае<sub>(GEOPOLIT)</sub></mark>, пишет <mark style=\"background-color: lightgreen\">The Washington Times<sub>(MEDIA)</sub></mark>.    По словам представителя <mark style=\"background-color: yellow\">Белого дома<sub>(GEOPOLIT)</sub></mark>, кандидатуру <mark style=\"background-color: pink\">Хантсмена<sub>(PER)</sub></mark> <mark style=\"background-color: pink\">Обаме<sub>(PER)</sub></mark> предложил помощник по азиатской политике <mark style=\"background-color: pink\">Джефф Бэйдер (Jeff Bader)<sub>(PER)</sub></mark>, который представил губернатора <mark style=\"background-color: lightblue\">Юты<sub>(LOC)</sub></mark> как человека, отлично знающего китайский язык, разбирающегося в проблемах региона и способного эффективно решать дипломатические задачи.    Члены <mark style=\"background-color: lightgrey\">Республиканской партии<sub>(ORG)</sub></mark>, в том числе и бывший соперник <mark style=\"background-color: pink\">Обамы<sub>(PER)</sub></mark> на выборах президента <mark style=\"background-color: yellow\">США<sub>(GEOPOLIT)</sub></mark> <mark style=\"background-color: pink\">Джон Маккейн<sub>(PER)</sub></mark>, поприветствовали выбор <mark style=\"background-color: pink\">Обамы<sub>(PER)</sub></mark>. Они, однако, в то же время признали, что в связи с этим назначением <mark style=\"background-color: pink\">Хантсмен<sub>(PER)</sub></mark> фактически лишился возможности участвовать в следующих президентских выборах.    Во время предвыборной кампании 2008 года <mark style=\"background-color: pink\">Хантсмен<sub>(PER)</sub></mark> был одним из руководителей штаба <mark style=\"background-color: pink\">Джона Маккейна<sub>(PER)</sub></mark> и, по оценкам экспертов, именно он мог стать кандидатом от республиканцев на следующих выборах. Многие республиканцы отметили, что, сделав <mark style=\"background-color: pink\">Хантсмена<sub>(PER)</sub></mark> послом, <mark style=\"background-color: pink\">Обама<sub>(PER)</sub></mark> устранил потенциально опасного соперника.    До избрания губернатором <mark style=\"background-color: lightblue\">Юты<sub>(LOC)</sub></mark> в 2004 году <mark style=\"background-color: pink\">Хантсмен<sub>(PER)</sub></mark> работал торговым представителем <mark style=\"background-color: yellow\">США<sub>(GEOPOLIT)</sub></mark> при <mark style=\"background-color: pink\">Джордже Буше-младшем<sub>(PER)</sub></mark>, а до этого был послом <mark style=\"background-color: yellow\">США<sub>(GEOPOLIT)</sub></mark> в <mark style=\"background-color: yellow\">Сингапуре<sub>(GEOPOLIT)</sub></mark>. "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_text_idx = 7\n",
    "print(test_data[_text_idx], '\\n')\n",
    "visualize_labeled_text(test_data[_text_idx], COLOR_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57805849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LabeledText(text='Hermitage: Главу налогового управления ГУВД Москвы уволили из-за С.Магнитского\\n\\nФонд Hermitage Capital связывает освобождение генерал-майора милиции Анатолия Михалкина от должности начальника управления по налоговым преступлениям ГУВД Москвы с делом юриста фонда Сергея Магнитского, умершего 16 ноября 2009г. в следственном изоляторе, говорится в распространенном сегодня сообщении Hermitage Capital.\\n\\nПо информации фонда, уволенный генерал-майор ГУВД Москвы был руководителем подполковника А.Кузнецова, против которого С.Магнитский давал показания. Кроме того, в июле-августе 2007г. А.Михалкин направил ряд запросов, подготовленных А.Кузнецовым, в московские филиалы международных банков с целью получения конфиденциальной информации о российских компаниях фонда Hermitage, утверждается в сообщении компании.', entities=[Ne5Span(start=0, stop=9, type='ORG'), Ne5Span(start=39, stop=43, type='ORG'), Ne5Span(start=44, stop=50, type='LOC'), Ne5Span(start=65, stop=78, type='PER'), Ne5Span(start=87, stop=104, type='ORG'), Ne5Span(start=151, stop=169, type='PER'), Ne5Span(start=232, stop=236, type='ORG'), Ne5Span(start=237, stop=243, type='LOC'), Ne5Span(start=265, stop=283, type='PER'), Ne5Span(start=384, stop=401, type='ORG'), Ne5Span(start=451, stop=455, type='ORG'), Ne5Span(start=456, stop=462, type='LOC'), Ne5Span(start=495, stop=506, type='PER'), Ne5Span(start=524, stop=536, type='PER'), Ne5Span(start=588, stop=598, type='PER'), Ne5Span(start=637, stop=649, type='PER'), Ne5Span(start=768, stop=777, type='ORG')]) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<mark style=\"background-color: lightgrey\">Hermitage<sub>(ORG)</sub></mark>: Главу налогового управления <mark style=\"background-color: lightgrey\">ГУВД<sub>(ORG)</sub></mark> <mark style=\"background-color: lightblue\">Москвы<sub>(LOC)</sub></mark> уволили из-за <mark style=\"background-color: pink\">С.Магнитского<sub>(PER)</sub></mark>    Фонд <mark style=\"background-color: lightgrey\">Hermitage Capital<sub>(ORG)</sub></mark> связывает освобождение генерал-майора милиции <mark style=\"background-color: pink\">Анатолия Михалкина<sub>(PER)</sub></mark> от должности начальника управления по налоговым преступлениям <mark style=\"background-color: lightgrey\">ГУВД<sub>(ORG)</sub></mark> <mark style=\"background-color: lightblue\">Москвы<sub>(LOC)</sub></mark> с делом юриста фонда <mark style=\"background-color: pink\">Сергея Магнитского<sub>(PER)</sub></mark>, умершего 16 ноября 2009г. в следственном изоляторе, говорится в распространенном сегодня сообщении <mark style=\"background-color: lightgrey\">Hermitage Capital<sub>(ORG)</sub></mark>.    По информации фонда, уволенный генерал-майор <mark style=\"background-color: lightgrey\">ГУВД<sub>(ORG)</sub></mark> <mark style=\"background-color: lightblue\">Москвы<sub>(LOC)</sub></mark> был руководителем подполковника <mark style=\"background-color: pink\">А.Кузнецова<sub>(PER)</sub></mark>, против которого <mark style=\"background-color: pink\">С.Магнитский<sub>(PER)</sub></mark> давал показания. Кроме того, в июле-августе 2007г. <mark style=\"background-color: pink\">А.Михалкин<sub>(PER)</sub></mark> направил ряд запросов, подготовленных <mark style=\"background-color: pink\">А.Кузнецовым<sub>(PER)</sub></mark>, в московские филиалы международных банков с целью получения конфиденциальной информации о российских компаниях фонда <mark style=\"background-color: lightgrey\">Hermitage<sub>(ORG)</sub></mark>, утверждается в сообщении компании."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_text_idx = 13\n",
    "\n",
    "print(test_data[_text_idx], '\\n')\n",
    "visualize_labeled_text(test_data[_text_idx], COLOR_MAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7065a4",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "# 2. Tokenize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aa89b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Токенизация и выравнивание меток\n",
    "def tokenize_rus(text: str) -> list[Substring]:\n",
    "    \"\"\"Implement word-wise tokenization for russian language.\"\"\"\n",
    "    return list(razdel.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ee93ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def symbol_bio_to_word_bio(\n",
    "    labeled_text: LabeledText,\n",
    "    subwords: list[Substring],\n",
    ") -> list[str]:\n",
    "    \"\"\"Transforms symbol-wise annotation to word-wise BIO notation for NER.\n",
    "\n",
    "    Args:\n",
    "        labeled_text: A LabeledText object containing the original text and\n",
    "            symbol-wise entity annotations.\n",
    "        subwords: A list of Substring objects representing the tokenized text.\n",
    "\n",
    "    Returns:\n",
    "        A list of strings representing BIO tags in the format:\n",
    "        - \"B-CAT\" for beginning of an entity of category CAT\n",
    "        - \"I-CAT\" for continuation of an entity of category CAT\n",
    "        - \"O\" for tokens outside any entity\n",
    "    \"\"\"\n",
    "    bio_tags = [\"O\"] * len(subwords)\n",
    "    entities = sorted(labeled_text.entities, key=lambda x: x.start)\n",
    "    for entity in entities:\n",
    "        overlapping = []\n",
    "        for i, subword in enumerate(subwords):\n",
    "            if (subword.start < entity.stop and subword.stop > entity.start):\n",
    "                overlapping.append(i)\n",
    "        for j, idx in enumerate(overlapping):\n",
    "            bio_tags[idx] = f\"B-{entity.type}\" if j == 0 else f\"I-{entity.type}\"\n",
    "    return bio_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "094bea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels: list[str], word_ids: list[str]) -> list[str]:\n",
    "    \"\"\"Aligns word-level BIO labels to token-level labels accounting for wordpiece tokenization.\n",
    "    \n",
    "    Args:\n",
    "        labels: List of BIO worl-level labels.\n",
    "        word_ids: List mapping each token to its original word index (from tokenizer.word_ids()).\n",
    "    \n",
    "    Returns:\n",
    "        List of aligned token-level BIO labels.\n",
    "    \"\"\"\n",
    "    new_labels = []\n",
    "    crnt_word_id = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id is None:\n",
    "            new_labels.append('Ignored')  # special token\n",
    "        elif word_id != crnt_word_id:\n",
    "            crnt_word_id = word_id\n",
    "            new_labels.append(labels[word_id])\n",
    "        else:\n",
    "            label = labels[word_id]\n",
    "            if label.startswith(\"B-\"):\n",
    "                label = f\"I-{label[2:]}\"\n",
    "            new_labels.append(label)\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "884a7bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelsTokenizerAligner:\n",
    "    def __init__(self, bio_labels_to_idx: dict[str, int], tokenizer):\n",
    "        self.bio_labels_to_idx = bio_labels_to_idx\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def _tokenize_and_align_labels(self, examples: Dataset):\n",
    "        \"\"\"Tokenizes input text and aligns word-level BIO labels with subword tokens.\n",
    "    \n",
    "        Args:\n",
    "            examples (Dataset): A Hugging Face `Dataset` object containing:\n",
    "                - `words`: List of words.\n",
    "                - `labels_bio`: List of word-level BIO labels.\n",
    "            tokenizer (PreTrainedTokenizer): Tokenizer (e.g., `BertTokenizer`) with subword tokenization.\n",
    "    \n",
    "        Returns:\n",
    "            (Dataset): A Hugging Face `Dataset` object.\n",
    "        \"\"\"\n",
    "        tokenized_inputs = self.tokenizer(\n",
    "            examples[\"words\"], \n",
    "            truncation=True, \n",
    "            padding=False,  # we will pad later with data_collator\n",
    "            is_split_into_words=True\n",
    "        )\n",
    "        aligned_labels = []\n",
    "        for sample_idx, bio_labels in enumerate(examples[\"bio_labels\"]):\n",
    "            word_ids = tokenized_inputs.word_ids(batch_index=sample_idx)\n",
    "            bio_labels_aligned = align_labels_with_tokens(bio_labels, word_ids)\n",
    "            aligned_labels.append([self.bio_labels_to_idx[bla] for bla in bio_labels_aligned])\n",
    "        tokenized_inputs[\"labels\"] = aligned_labels\n",
    "        return tokenized_inputs\n",
    "\n",
    "    def __call__(self, examples: Dataset):\n",
    "        return self._tokenize_and_align_labels(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7223061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tokenized_dataset(\n",
    "    ds_ner: list[LabeledText],\n",
    "    bio_labels_to_idx: dict[str, int],\n",
    "    tokenizer,\n",
    ") -> Dataset:\n",
    "    \"\"\"Make tokenized dataset ready for batching.\n",
    "\n",
    "    Makes(Dataset): a Hugging Face `Dataset` object containing model inputs:\n",
    "        input_ids, token_type_ids, and an attention_mask via the steps:\n",
    "        1) Represent raw dataset as python dictionary of words and bio_labels.\n",
    "        2) Split sentences into words and convert symbol-wise labeling into word-wise BIO format.\n",
    "        3) Tokenize words and align the labels with sub-tokens level.\n",
    "\n",
    "    Args:\n",
    "        ds_ner: List of (LabeledText) objects.\n",
    "        bio_labels_to_idx: Map for BIO labels to int including 'Ignored' key for special tokens.\n",
    "        tokenizer (PreTrainedTokenizer): Tokenizer (e.g., `BertTokenizer`) with subword tokenization.\n",
    "\n",
    "    Returns:\n",
    "        (Dataset): A tokenized Hugging Face `Dataset` object.\n",
    "    \"\"\"\n",
    "    ds_dict = {\"words\": [], \"bio_labels\": []}\n",
    "    for sample in ds_ner:\n",
    "        words_subs = tokenize_rus(sample.text)\n",
    "        words = [ws.text for ws in words_subs]\n",
    "        bio_labels = symbol_bio_to_word_bio(sample, words_subs)\n",
    "        ds_dict[\"words\"].append(words)\n",
    "        ds_dict[\"bio_labels\"].append(bio_labels)\n",
    "    dataset = Dataset.from_dict(ds_dict)\n",
    "    labels_tokenizer_aligner = LabelsTokenizerAligner(bio_labels_to_idx, tokenizer)\n",
    "    dataset_tokenized = dataset.map(             # The tokenizer generates three new columns in the\n",
    "        labels_tokenizer_aligner,                # dataset: input_ids, token_type_ids, and an\n",
    "        batched=True,                            # attention_mask. These are the model inputs.\n",
    "        remove_columns=['words', 'bio_labels'],  # <-- We should delete the rest as we don't need them.\n",
    "    )\n",
    "    return dataset_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8281eac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-GEOPOLIT': 1,\n",
       " 'I-GEOPOLIT': 2,\n",
       " 'B-LOC': 3,\n",
       " 'I-LOC': 4,\n",
       " 'B-MEDIA': 5,\n",
       " 'I-MEDIA': 6,\n",
       " 'B-PER': 7,\n",
       " 'I-PER': 8,\n",
       " 'B-ORG': 9,\n",
       " 'I-ORG': 10,\n",
       " 'Ignored': -100}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bio_labels_to_idx = {'O': 0}\n",
    "for cat in categories_map:\n",
    "    for prefix in 'BI':\n",
    "        bio_labels_to_idx[f'{prefix}-{cat}'] = len(bio_labels_to_idx)\n",
    "bio_labels_to_idx['Ignored'] = -100\n",
    "\n",
    "idx_to_bio_labels = {lbl: bio for bio, lbl in bio_labels_to_idx.items()}\n",
    "\n",
    "bio_labels_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb9771d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготавливаем tokenizer и датасеты\n",
    "model_checkpoint = \"cointegrated/rubert-tiny2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5cd73687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c3834f72fa64acca7aa9e498962dee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ea3b04282624557a7786971ceea41db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_tokenized_test = make_tokenized_dataset(test_data, bio_labels_to_idx, tokenizer)\n",
    "ds_tokenized_train = make_tokenized_dataset(train_data, bio_labels_to_idx, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05e33358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test size: 200\n",
      "train size: 800\n"
     ]
    }
   ],
   "source": [
    "print(f'test size: {len(ds_tokenized_test)}')\n",
    "print(f'train size: {len(ds_tokenized_train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82ac83bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw test: 4 октября назначены очередные выборы Верховного Совета Аджарской АР\n",
      "\n",
      "По распоряжению президента Грузии Михаила Саакашвили 4 октября 2008 года назначены очередные выборы Верховного Совета Аджарской АР. Об этом Новости-Грузия сообщили в пресс-службе администрации президента во вторник.\n",
      "\n",
      "Выборы Верховного совета Аджарской автономной республики назначены в соответствии с 241-ой статьей и 4-м пунктом 10-й статьи Конституционного закона Грузии <О статусе Аджарской автономной республики>.\n",
      "tok test: 4 октября назначены очередные выборы Верховного Совета Аджарской АР По распоряжению президента Грузии Михаила Саакашвили 4 октября 2008 года назначены очередные выборы Верховного Совета Аджарской АР. Об этом Новости - Грузия сообщили в пресс - службе администрации президента во вторник. Выборы Верховного совета Аджарской автономной республики назначены в соответствии с 241 - ой статьей и 4 - м пунктом 10 - й статьи Конституционного закона Грузии < О статусе Аджарской автономной республики >.\n"
     ]
    }
   ],
   "source": [
    "_test_idx = 0\n",
    "print(f'raw test: {test_data[_test_idx].text}')\n",
    "_tokenized_decoded = tokenizer.decode(ds_tokenized_test[_test_idx]['input_ids'], skip_special_tokens=True)\n",
    "print(f\"tok test: {_tokenized_decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024d84c2",
   "metadata": {},
   "source": [
    "# Дообучение модели rubert-tiny2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad13586",
   "metadata": {},
   "source": [
    "## 3.1 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8df44fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b111e591",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {label: idx for label, idx in bio_labels_to_idx.items() if idx >= 0}\n",
    "id2label = {idx: label for label, idx in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ed0bc43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-GEOPOLIT',\n",
       " 2: 'I-GEOPOLIT',\n",
       " 3: 'B-LOC',\n",
       " 4: 'I-LOC',\n",
       " 5: 'B-MEDIA',\n",
       " 6: 'I-MEDIA',\n",
       " 7: 'B-PER',\n",
       " 8: 'I-PER',\n",
       " 9: 'B-ORG',\n",
       " 10: 'I-ORG'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e5426f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12b49c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d346eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"seqeval\")\n",
    "label_names = list(label2id.keys()) \n",
    "\n",
    "def compute_metrics(eval_preds: tuple[np.ndarray, np.ndarray]) -> dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics for token classification (NER) using seqeval.\n",
    "\n",
    "    Args:\n",
    "        eval_preds (tuple): A tuple containing:\n",
    "            - logits (np.ndarray): Model output logits of shape (batch_size, seq_len, num_labels)\n",
    "            - labels (np.ndarray): Ground truth label ids of shape (batch_size, seq_len)\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with overall precision, recall, F1 score, and accuracy.\n",
    "    \"\"\"\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    metrics = metric.compute(predictions=true_predictions, references=true_labels, zero_division=0)\n",
    "\n",
    "    return {\n",
    "        \"precision\": metrics[\"overall_precision\"],\n",
    "        \"recall\": metrics[\"overall_recall\"],\n",
    "        \"f1\": metrics[\"overall_f1\"],\n",
    "        \"accuracy\": metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf73b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# — оптимизатор AdamW (по умолчанию в HF Trainer)\n",
    "# — lr=2e-5 — стандарт для дообучения BERT-подобных\n",
    "# — batch_size=8 — укладывается в 8–12 GB VRAM\n",
    "# — epochs=20 — даёт стабильную сходимость на малых корпусах\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=25,\n",
    "    weight_decay=0.01,\n",
    "    seed=random_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "317af0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds_tokenized_train,\n",
    "    eval_dataset=ds_tokenized_test,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ed32f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрики до дообучения:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.4798085689544678,\n",
       " 'eval_model_preparation_time': 0.0013,\n",
       " 'eval_precision': 0.002396244574077348,\n",
       " 'eval_recall': 0.02341201304931875,\n",
       " 'eval_f1': 0.004347516214097356,\n",
       " 'eval_accuracy': 0.04799117546530777,\n",
       " 'eval_runtime': 1.7207,\n",
       " 'eval_samples_per_second': 116.234,\n",
       " 'eval_steps_per_second': 14.529}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Метрики до дообучения:\")\n",
    "trainer.evaluate(ds_tokenized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "24eaedd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 05:23, Epoch 25/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.880844</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.101950</td>\n",
       "      <td>0.048167</td>\n",
       "      <td>0.065424</td>\n",
       "      <td>0.763291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.712447</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.128584</td>\n",
       "      <td>0.085204</td>\n",
       "      <td>0.102493</td>\n",
       "      <td>0.794565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.620033</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.171195</td>\n",
       "      <td>0.136634</td>\n",
       "      <td>0.151974</td>\n",
       "      <td>0.814343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.560742</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.200128</td>\n",
       "      <td>0.180196</td>\n",
       "      <td>0.189640</td>\n",
       "      <td>0.827145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.806500</td>\n",
       "      <td>0.516552</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.235746</td>\n",
       "      <td>0.211860</td>\n",
       "      <td>0.223166</td>\n",
       "      <td>0.836762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.806500</td>\n",
       "      <td>0.487550</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.247217</td>\n",
       "      <td>0.247169</td>\n",
       "      <td>0.247193</td>\n",
       "      <td>0.843054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.806500</td>\n",
       "      <td>0.460214</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.264335</td>\n",
       "      <td>0.269814</td>\n",
       "      <td>0.267047</td>\n",
       "      <td>0.849330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.806500</td>\n",
       "      <td>0.440893</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.281449</td>\n",
       "      <td>0.287661</td>\n",
       "      <td>0.284521</td>\n",
       "      <td>0.854069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.806500</td>\n",
       "      <td>0.424094</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.296262</td>\n",
       "      <td>0.305699</td>\n",
       "      <td>0.300907</td>\n",
       "      <td>0.859522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.459700</td>\n",
       "      <td>0.411086</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.304723</td>\n",
       "      <td>0.318173</td>\n",
       "      <td>0.311303</td>\n",
       "      <td>0.862023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.459700</td>\n",
       "      <td>0.404732</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.308288</td>\n",
       "      <td>0.328344</td>\n",
       "      <td>0.318000</td>\n",
       "      <td>0.863111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.459700</td>\n",
       "      <td>0.399435</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.307008</td>\n",
       "      <td>0.331222</td>\n",
       "      <td>0.318656</td>\n",
       "      <td>0.863763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.459700</td>\n",
       "      <td>0.390241</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.317233</td>\n",
       "      <td>0.335252</td>\n",
       "      <td>0.325994</td>\n",
       "      <td>0.867057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.459700</td>\n",
       "      <td>0.389752</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.321232</td>\n",
       "      <td>0.346383</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.866809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.366800</td>\n",
       "      <td>0.382368</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.347342</td>\n",
       "      <td>0.334504</td>\n",
       "      <td>0.869434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.366800</td>\n",
       "      <td>0.378283</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.327403</td>\n",
       "      <td>0.349645</td>\n",
       "      <td>0.338159</td>\n",
       "      <td>0.869885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.366800</td>\n",
       "      <td>0.376368</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.328574</td>\n",
       "      <td>0.353291</td>\n",
       "      <td>0.340485</td>\n",
       "      <td>0.870304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.366800</td>\n",
       "      <td>0.374105</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.330667</td>\n",
       "      <td>0.356937</td>\n",
       "      <td>0.343300</td>\n",
       "      <td>0.871143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.366800</td>\n",
       "      <td>0.372954</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.328353</td>\n",
       "      <td>0.358472</td>\n",
       "      <td>0.342752</td>\n",
       "      <td>0.871267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.326100</td>\n",
       "      <td>0.371429</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.330689</td>\n",
       "      <td>0.360008</td>\n",
       "      <td>0.344726</td>\n",
       "      <td>0.871656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.326100</td>\n",
       "      <td>0.370124</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.331910</td>\n",
       "      <td>0.357897</td>\n",
       "      <td>0.344414</td>\n",
       "      <td>0.872075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.326100</td>\n",
       "      <td>0.370511</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.334631</td>\n",
       "      <td>0.362886</td>\n",
       "      <td>0.348186</td>\n",
       "      <td>0.872386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.326100</td>\n",
       "      <td>0.368775</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.335578</td>\n",
       "      <td>0.363462</td>\n",
       "      <td>0.348964</td>\n",
       "      <td>0.872697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.326100</td>\n",
       "      <td>0.369449</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.333040</td>\n",
       "      <td>0.362694</td>\n",
       "      <td>0.347235</td>\n",
       "      <td>0.872091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.307300</td>\n",
       "      <td>0.368764</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.332335</td>\n",
       "      <td>0.362119</td>\n",
       "      <td>0.346588</td>\n",
       "      <td>0.872200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2500, training_loss=0.45330238647460935, metrics={'train_runtime': 323.1975, 'train_samples_per_second': 61.882, 'train_steps_per_second': 7.735, 'total_flos': 177478250391600.0, 'train_loss': 0.45330238647460935, 'epoch': 25.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2853b8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрики после дообучения:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3687644302845001,\n",
       " 'eval_model_preparation_time': 0.0013,\n",
       " 'eval_precision': 0.3323353293413174,\n",
       " 'eval_recall': 0.362118595279217,\n",
       " 'eval_f1': 0.34658830011938657,\n",
       " 'eval_accuracy': 0.8721996084889538,\n",
       " 'eval_runtime': 1.3794,\n",
       " 'eval_samples_per_second': 144.992,\n",
       " 'eval_steps_per_second': 18.124,\n",
       " 'epoch': 25.0}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Метрики после дообучения:\")\n",
    "trainer.evaluate(ds_tokenized_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43d0ffa",
   "metadata": {},
   "source": [
    "## 3.2 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "951f41b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>O</th>\n",
       "      <th>B-GEOPOLIT</th>\n",
       "      <th>I-GEOPOLIT</th>\n",
       "      <th>B-LOC</th>\n",
       "      <th>I-LOC</th>\n",
       "      <th>B-MEDIA</th>\n",
       "      <th>I-MEDIA</th>\n",
       "      <th>B-PER</th>\n",
       "      <th>I-PER</th>\n",
       "      <th>B-ORG</th>\n",
       "      <th>I-ORG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>43495</td>\n",
       "      <td>97</td>\n",
       "      <td>102</td>\n",
       "      <td>59</td>\n",
       "      <td>261</td>\n",
       "      <td>39</td>\n",
       "      <td>89</td>\n",
       "      <td>344</td>\n",
       "      <td>1052</td>\n",
       "      <td>266</td>\n",
       "      <td>713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-GEOPOLIT</th>\n",
       "      <td>137</td>\n",
       "      <td>392</td>\n",
       "      <td>51</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-GEOPOLIT</th>\n",
       "      <td>147</td>\n",
       "      <td>58</td>\n",
       "      <td>225</td>\n",
       "      <td>3</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-LOC</th>\n",
       "      <td>81</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>344</td>\n",
       "      <td>124</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-LOC</th>\n",
       "      <td>197</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>22</td>\n",
       "      <td>1087</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-MEDIA</th>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>167</td>\n",
       "      <td>33</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-MEDIA</th>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>400</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-PER</th>\n",
       "      <td>292</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1373</td>\n",
       "      <td>515</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-PER</th>\n",
       "      <td>669</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>243</td>\n",
       "      <td>5580</td>\n",
       "      <td>9</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-ORG</th>\n",
       "      <td>334</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>11</td>\n",
       "      <td>673</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-ORG</th>\n",
       "      <td>633</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>30</td>\n",
       "      <td>107</td>\n",
       "      <td>113</td>\n",
       "      <td>2404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                O  B-GEOPOLIT  I-GEOPOLIT  B-LOC  I-LOC  B-MEDIA  I-MEDIA  \\\n",
       "O           43495          97         102     59    261       39       89   \n",
       "B-GEOPOLIT    137         392          51     17     10        1        0   \n",
       "I-GEOPOLIT    147          58         225      3     41        0        0   \n",
       "B-LOC          81          22           5    344    124        0        0   \n",
       "I-LOC         197           4          28     22   1087        0        0   \n",
       "B-MEDIA        71           0           0      2      0      167       33   \n",
       "I-MEDIA       106           0           2      0      2       22      400   \n",
       "B-PER         292           4           2      8      6        0        3   \n",
       "I-PER         669           2           6      2     21        3        2   \n",
       "B-ORG         334          17           0     21      2        8        3   \n",
       "I-ORG         633          11          12      7     72        0       22   \n",
       "\n",
       "            B-PER  I-PER  B-ORG  I-ORG  \n",
       "O             344   1052    266    713  \n",
       "B-GEOPOLIT     23      7     14     46  \n",
       "I-GEOPOLIT      6     41      1     61  \n",
       "B-LOC          26     10     13     12  \n",
       "I-LOC          13     36      7     47  \n",
       "B-MEDIA         5      0     12      5  \n",
       "I-MEDIA        11     14      1     77  \n",
       "B-PER        1373    515     12     19  \n",
       "I-PER         243   5580      9     31  \n",
       "B-ORG          23     11    673    255  \n",
       "I-ORG          30    107    113   2404  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_output = trainer.predict(ds_tokenized_test)\n",
    "logits = predictions_output.predictions\n",
    "labels = predictions_output.label_ids\n",
    "preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "pred_labels = [[label_names[p] for (p, l) in zip(pred, label) if l != -100] for pred, label in zip(preds, labels)]\n",
    "\n",
    "y_true_flat = [l for seq in true_labels for l in seq]\n",
    "y_pred_flat = [l for seq in pred_labels for l in seq]\n",
    "\n",
    "cm = confusion_matrix(y_true_flat, y_pred_flat, labels=label_names)\n",
    "cm_df = pd.DataFrame(cm, index=label_names, columns=label_names)\n",
    "print(\"Confusion Matrix:\")\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e9f903ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    GEOPOLIT       0.39      0.40      0.40       698\n",
      "         LOC       0.37      0.38      0.37       637\n",
      "       MEDIA       0.35      0.37      0.36       295\n",
      "         ORG       0.25      0.28      0.26      1347\n",
      "         PER       0.35      0.39      0.37      2234\n",
      "\n",
      "   micro avg       0.33      0.36      0.35      5211\n",
      "   macro avg       0.34      0.36      0.35      5211\n",
      "weighted avg       0.33      0.36      0.35      5211\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "23004a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.9145874, 'word': 'Барак Обама', 'start': 0, 'end': 11}, {'entity_group': 'PER', 'score': 0.45619935, 'word': 'Ю', 'start': 33, 'end': 34}, {'entity_group': 'GEOPOLIT', 'score': 0.55067486, 'word': '##ты', 'start': 34, 'end': 36}, {'entity_group': 'GEOPOLIT', 'score': 0.8653656, 'word': 'США', 'start': 44, 'end': 47}, {'entity_group': 'GEOPOLIT', 'score': 0.8083028, 'word': 'Китае', 'start': 50, 'end': 55}, {'entity_group': 'GEOPOLIT', 'score': 0.85899687, 'word': 'США', 'start': 66, 'end': 69}, {'entity_group': 'PER', 'score': 0.84993076, 'word': 'Барак Обама 16', 'start': 70, 'end': 84}, {'entity_group': 'PER', 'score': 0.84370923, 'word': 'Ю', 'start': 116, 'end': 117}, {'entity_group': 'LOC', 'score': 0.29862353, 'word': '##та', 'start': 117, 'end': 119}, {'entity_group': 'PER', 'score': 0.93264985, 'word': 'Джона Хантсмена - младшего ( John Huntsman Jr. )', 'start': 120, 'end': 164}, {'entity_group': 'GEOPOLIT', 'score': 0.5199127, 'word': 'США в', 'start': 172, 'end': 177}, {'entity_group': 'GEOPOLIT', 'score': 0.74127215, 'word': 'Китае,', 'start': 178, 'end': 184}, {'entity_group': 'MEDIA', 'score': 0.6197525, 'word': 'The', 'start': 191, 'end': 194}, {'entity_group': 'MEDIA', 'score': 0.817815, 'word': 'Washington Times.', 'start': 195, 'end': 212}, {'entity_group': 'GEOPOLIT', 'score': 0.35307497, 'word': 'Белого', 'start': 237, 'end': 243}, {'entity_group': 'LOC', 'score': 0.5048803, 'word': 'дома', 'start': 244, 'end': 248}, {'entity_group': 'GEOPOLIT', 'score': 0.66469836, 'word': ',', 'start': 248, 'end': 249}, {'entity_group': 'PER', 'score': 0.9193298, 'word': 'Хантсмена Обаме предложил', 'start': 262, 'end': 287}, {'entity_group': 'PER', 'score': 0.9677254, 'word': 'Джефф Бэйдер ( Jeff Bader ), который', 'start': 319, 'end': 353}, {'entity_group': 'ORG', 'score': 0.8930783, 'word': 'Республиканской партии, в том', 'start': 524, 'end': 553}, {'entity_group': 'PER', 'score': 0.80849063, 'word': 'на выборах', 'start': 584, 'end': 594}, {'entity_group': 'PER', 'score': 0.58332425, 'word': 'Джон', 'start': 610, 'end': 614}, {'entity_group': 'PER', 'score': 0.9379265, 'word': 'Маккейн, поприветствовали', 'start': 615, 'end': 640}, {'entity_group': 'PER', 'score': 0.7964568, 'word': '. Они, однако', 'start': 652, 'end': 665}, {'entity_group': 'PER', 'score': 0.75292635, 'word': '##тсмен фактически лишился', 'start': 725, 'end': 749}, {'entity_group': 'PER', 'score': 0.3845185, 'word': '##мен', 'start': 855, 'end': 858}, {'entity_group': 'PER', 'score': 0.6211716, 'word': 'был одним', 'start': 859, 'end': 868}, {'entity_group': 'PER', 'score': 0.9543889, 'word': 'Маккейна и, по', 'start': 898, 'end': 912}, {'entity_group': 'PER', 'score': 0.65974885, 'word': '##мена послом', 'start': 1051, 'end': 1062}, {'entity_group': 'PER', 'score': 0.53967655, 'word': 'устранил', 'start': 1070, 'end': 1078}, {'entity_group': 'PER', 'score': 0.7005149, 'word': 'работал', 'start': 1162, 'end': 1169}, {'entity_group': 'GEOPOLIT', 'score': 0.4533952, 'word': 'Джордже', 'start': 1202, 'end': 1209}, {'entity_group': 'PER', 'score': 0.829906, 'word': 'Буше - младшем, а', 'start': 1210, 'end': 1225}, {'entity_group': 'GEOPOLIT', 'score': 0.49647903, 'word': 'Сингапуре.', 'start': 1252, 'end': 1262}]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<mark style=\"background-color: pink\">Барак Обама<sub>(PER)</sub></mark> назначил губернатора <mark style=\"background-color: pink\">Ю<sub>(PER)</sub></mark><mark style=\"background-color: yellow\">ты<sub>(GEOPOLIT)</sub></mark> послом <mark style=\"background-color: yellow\">США<sub>(GEOPOLIT)</sub></mark> в <mark style=\"background-color: yellow\">Китае<sub>(GEOPOLIT)</sub></mark> Президент <mark style=\"background-color: yellow\">США<sub>(GEOPOLIT)</sub></mark> <mark style=\"background-color: pink\">Барак Обама 16<sub>(PER)</sub></mark> мая назначил губернатора штата <mark style=\"background-color: pink\">Ю<sub>(PER)</sub></mark><mark style=\"background-color: lightblue\">та<sub>(LOC)</sub></mark> <mark style=\"background-color: pink\">Джона Хантсмена-младшего (John Huntsman Jr.)<sub>(PER)</sub></mark> послом <mark style=\"background-color: yellow\">США в<sub>(GEOPOLIT)</sub></mark> <mark style=\"background-color: yellow\">Китае,<sub>(GEOPOLIT)</sub></mark> пишет <mark style=\"background-color: lightgreen\">The<sub>(MEDIA)</sub></mark> <mark style=\"background-color: lightgreen\">Washington Times.<sub>(MEDIA)</sub></mark> По словам представителя <mark style=\"background-color: yellow\">Белого<sub>(GEOPOLIT)</sub></mark> <mark style=\"background-color: lightblue\">дома<sub>(LOC)</sub></mark><mark style=\"background-color: yellow\">,<sub>(GEOPOLIT)</sub></mark> кандидатуру <mark style=\"background-color: pink\">Хантсмена Обаме предложил<sub>(PER)</sub></mark> помощник по азиатской политике <mark style=\"background-color: pink\">Джефф Бэйдер (Jeff Bader), который<sub>(PER)</sub></mark> представил губернатора Юты как человека, отлично знающего китайский язык, разбирающегося в проблемах региона и способного эффективно решать дипломатические задачи. Члены <mark style=\"background-color: lightgrey\">Республиканской партии, в том<sub>(ORG)</sub></mark> числе и бывший соперник Обамы <mark style=\"background-color: pink\">на выборах<sub>(PER)</sub></mark> президента США <mark style=\"background-color: pink\">Джон<sub>(PER)</sub></mark> <mark style=\"background-color: pink\">Маккейн, поприветствовали<sub>(PER)</sub></mark> выбор Обамы<mark style=\"background-color: pink\">. Они, однако<sub>(PER)</sub></mark>, в то же время признали, что в связи с этим назначением Хан<mark style=\"background-color: pink\">тсмен фактически лишился<sub>(PER)</sub></mark> возможности участвовать в следующих президентских выборах. Во время предвыборной кампании 2008 года Хантс<mark style=\"background-color: pink\">мен<sub>(PER)</sub></mark> <mark style=\"background-color: pink\">был одним<sub>(PER)</sub></mark> из руководителей штаба Джона <mark style=\"background-color: pink\">Маккейна и, по<sub>(PER)</sub></mark> оценкам экспертов, именно он мог стать кандидатом от республиканцев на следующих выборах. Многие республиканцы отметили, что, сделав Хантс<mark style=\"background-color: pink\">мена послом<sub>(PER)</sub></mark>, Обама <mark style=\"background-color: pink\">устранил<sub>(PER)</sub></mark> потенциально опасного соперника. До избрания губернатором Юты в 2004 году Хантсмен <mark style=\"background-color: pink\">работал<sub>(PER)</sub></mark> торговым представителем США при <mark style=\"background-color: yellow\">Джордже<sub>(GEOPOLIT)</sub></mark> <mark style=\"background-color: pink\">Буше-младшем, а<sub>(PER)</sub></mark> до этого был послом США в <mark style=\"background-color: yellow\">Сингапуре.<sub>(GEOPOLIT)</sub></mark>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\", device=0)\n",
    "\n",
    "text = \"Барак Обама назначил губернатора Юты послом США в Китае Президент США Барак Обама 16 мая назначил губернатора штата Юта Джона Хантсмена-младшего (John Huntsman Jr.) послом США в Китае, пишет The Washington Times. По словам представителя Белого дома, кандидатуру Хантсмена Обаме предложил помощник по азиатской политике Джефф Бэйдер (Jeff Bader), который представил губернатора Юты как человека, отлично знающего китайский язык, разбирающегося в проблемах региона и способного эффективно решать дипломатические задачи. Члены Республиканской партии, в том числе и бывший соперник Обамы на выборах президента США Джон Маккейн, поприветствовали выбор Обамы. Они, однако, в то же время признали, что в связи с этим назначением Хантсмен фактически лишился возможности участвовать в следующих президентских выборах. Во время предвыборной кампании 2008 года Хантсмен был одним из руководителей штаба Джона Маккейна и, по оценкам экспертов, именно он мог стать кандидатом от республиканцев на следующих выборах. Многие республиканцы отметили, что, сделав Хантсмена послом, Обама устранил потенциально опасного соперника. До избрания губернатором Юты в 2004 году Хантсмен работал торговым представителем США при Джордже Буше-младшем, а до этого был послом США в Сингапуре.\"\n",
    "entities = ner_pipeline(text)\n",
    "print(entities)\n",
    "\n",
    "predicted_entities = [\n",
    "    Ne5Span(start=ent[\"start\"], stop=ent[\"end\"], type=ent[\"entity_group\"])\n",
    "    for ent in entities\n",
    "]\n",
    "\n",
    "predicted_labeled_text = LabeledText(text=text, entities=predicted_entities)\n",
    "\n",
    "visualize_labeled_text(predicted_labeled_text, COLOR_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b9dbc677",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./ner_finetuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a1423aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "del trainer\n",
    "del tokenizer\n",
    "del data_collator\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e0dd8d",
   "metadata": {},
   "source": [
    "# 4. Дообучение в MLM режиме"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbdca3b",
   "metadata": {},
   "source": [
    "## 4.1 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9359d717",
   "metadata": {},
   "source": [
    "- цель: донастроить LM-часть модели на доменных текстах перед дообучением NER\n",
    "- block_size=512 — максимальный контекст для Rubert-tiny2\n",
    "- mlm_probability=0.15 — по умолчанию в BERT\n",
    "- повторная fine-tuning тех же параметров lr/batch/epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e0149db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 512\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of block_size.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b1854f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f01faafb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dbc545947b84c489fa299e9a5d235b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2317567878234db7a00f85b305eba674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_tokenized_mlm = ds_tokenized_train.map(group_texts, batched=True)\n",
    "ds_tokenized_mlm_test = ds_tokenized_test.map(group_texts, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5b4fcf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d4071c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_args  = TrainingArguments(\n",
    "    output_dir=\"./mlm_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=25,\n",
    "    weight_decay=0.01,\n",
    "    seed=random_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c3a42b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=mlm_args ,\n",
    "    train_dataset=ds_tokenized_mlm,\n",
    "    eval_dataset=ds_tokenized_mlm, #использую тоже трейн чтобы не было утечки\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1bf7f572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='77' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 3.0680694580078125,\n",
       " 'eval_model_preparation_time': 0.002,\n",
       " 'eval_runtime': 1.0423,\n",
       " 'eval_samples_per_second': 120.885,\n",
       " 'eval_steps_per_second': 15.35}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm_trainer.evaluate(ds_tokenized_mlm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bc037826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1525' max='1525' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1525/1525 05:33, Epoch 25/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.058262</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.013006</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.933980</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.888930</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.864507</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.883245</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.849713</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.804169</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.225500</td>\n",
       "      <td>2.817735</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.225500</td>\n",
       "      <td>2.763472</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.225500</td>\n",
       "      <td>2.762005</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.225500</td>\n",
       "      <td>2.733955</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>3.225500</td>\n",
       "      <td>2.715899</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>3.225500</td>\n",
       "      <td>2.726694</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>3.225500</td>\n",
       "      <td>2.716736</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>3.225500</td>\n",
       "      <td>2.710847</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>3.036100</td>\n",
       "      <td>2.681430</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.036100</td>\n",
       "      <td>2.696490</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>3.036100</td>\n",
       "      <td>2.667386</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.036100</td>\n",
       "      <td>2.668224</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>3.036100</td>\n",
       "      <td>2.661503</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>3.036100</td>\n",
       "      <td>2.663334</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>3.036100</td>\n",
       "      <td>2.677709</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>3.036100</td>\n",
       "      <td>2.667444</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.969400</td>\n",
       "      <td>2.641651</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1525, training_loss=3.0757911857229763, metrics={'train_runtime': 333.2921, 'train_samples_per_second': 36.379, 'train_steps_per_second': 4.576, 'total_flos': 92534610432000.0, 'train_loss': 3.0757911857229763, 'epoch': 25.0})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e0037957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='61' max='61' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [61/61 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.6637213230133057,\n",
       " 'eval_model_preparation_time': 0.002,\n",
       " 'eval_runtime': 3.3615,\n",
       " 'eval_samples_per_second': 144.28,\n",
       " 'eval_steps_per_second': 18.147,\n",
       " 'epoch': 25.0}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4e6209cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_trainer.save_model(\"./mlm_model\")\n",
    "del mlm_trainer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace27c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at ./mlm_model and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Постобучение NER после MLM\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"./mlm_model\", num_labels=len(id2label), id2label=id2label, label2id=label2id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b8b1ecba",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_args  = TrainingArguments(\n",
    "    output_dir=\"./post_mlm_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=25,\n",
    "    weight_decay=0.01,\n",
    "    seed=random_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "61d01a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_mlm_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=post_args ,\n",
    "    train_dataset=ds_tokenized_train,\n",
    "    eval_dataset=ds_tokenized_test, \n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "247b9187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрики до дообучения:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.0026155591046918038,\n",
       " 'recall': 0.021684897332565727,\n",
       " 'f1': 0.004668071219068865,\n",
       " 'accuracy': 0.12991330826834044}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Метрики до дообучения:\")\n",
    "post_mlm_trainer.evaluate(ds_tokenized_test)\n",
    "compute_metrics(\n",
    "    (post_mlm_trainer.predict(ds_tokenized_test).predictions, ds_tokenized_test['labels'])\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3a9978cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 05:10, Epoch 25/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.864322</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.689883</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.601047</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.540374</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.784600</td>\n",
       "      <td>0.495677</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.784600</td>\n",
       "      <td>0.465287</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.784600</td>\n",
       "      <td>0.439280</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.784600</td>\n",
       "      <td>0.421655</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.784600</td>\n",
       "      <td>0.408100</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.440500</td>\n",
       "      <td>0.397149</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.440500</td>\n",
       "      <td>0.390707</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.440500</td>\n",
       "      <td>0.385968</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.440500</td>\n",
       "      <td>0.379200</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.440500</td>\n",
       "      <td>0.377872</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.353000</td>\n",
       "      <td>0.372036</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.353000</td>\n",
       "      <td>0.368891</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.353000</td>\n",
       "      <td>0.366184</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.353000</td>\n",
       "      <td>0.364658</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.353000</td>\n",
       "      <td>0.364443</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.313700</td>\n",
       "      <td>0.362526</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.313700</td>\n",
       "      <td>0.361593</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.313700</td>\n",
       "      <td>0.361217</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.313700</td>\n",
       "      <td>0.360161</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.313700</td>\n",
       "      <td>0.360536</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.296200</td>\n",
       "      <td>0.360416</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2500, training_loss=0.4376122497558594, metrics={'train_runtime': 310.1322, 'train_samples_per_second': 64.489, 'train_steps_per_second': 8.061, 'total_flos': 177478250391600.0, 'train_loss': 0.4376122497558594, 'epoch': 25.0})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_mlm_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b995469c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрики после дообучения:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.3410631466286122,\n",
       " 'recall': 0.3669161389368643,\n",
       " 'f1': 0.3535176111676065,\n",
       " 'accuracy': 0.8747475375198086}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Метрики после дообучения:\")\n",
    "post_mlm_trainer.evaluate()\n",
    "compute_metrics(\n",
    "    (post_mlm_trainer.predict(ds_tokenized_test).predictions, ds_tokenized_test['labels'])\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d6d272",
   "metadata": {},
   "source": [
    "## 4.2 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "09086d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>O</th>\n",
       "      <th>B-GEOPOLIT</th>\n",
       "      <th>I-GEOPOLIT</th>\n",
       "      <th>B-LOC</th>\n",
       "      <th>I-LOC</th>\n",
       "      <th>B-MEDIA</th>\n",
       "      <th>I-MEDIA</th>\n",
       "      <th>B-PER</th>\n",
       "      <th>I-PER</th>\n",
       "      <th>B-ORG</th>\n",
       "      <th>I-ORG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>43683</td>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "      <td>48</td>\n",
       "      <td>237</td>\n",
       "      <td>32</td>\n",
       "      <td>106</td>\n",
       "      <td>286</td>\n",
       "      <td>983</td>\n",
       "      <td>240</td>\n",
       "      <td>692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-GEOPOLIT</th>\n",
       "      <td>132</td>\n",
       "      <td>388</td>\n",
       "      <td>59</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-GEOPOLIT</th>\n",
       "      <td>152</td>\n",
       "      <td>57</td>\n",
       "      <td>230</td>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-LOC</th>\n",
       "      <td>96</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>347</td>\n",
       "      <td>116</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-LOC</th>\n",
       "      <td>202</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>1109</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-MEDIA</th>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>159</td>\n",
       "      <td>43</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-MEDIA</th>\n",
       "      <td>107</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>427</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-PER</th>\n",
       "      <td>313</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1343</td>\n",
       "      <td>498</td>\n",
       "      <td>14</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-PER</th>\n",
       "      <td>687</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>246</td>\n",
       "      <td>5550</td>\n",
       "      <td>7</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-ORG</th>\n",
       "      <td>358</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>673</td>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-ORG</th>\n",
       "      <td>667</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>32</td>\n",
       "      <td>93</td>\n",
       "      <td>100</td>\n",
       "      <td>2395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                O  B-GEOPOLIT  I-GEOPOLIT  B-LOC  I-LOC  B-MEDIA  I-MEDIA  \\\n",
       "O           43683         105         105     48    237       32      106   \n",
       "B-GEOPOLIT    132         388          59     18      9        0        0   \n",
       "I-GEOPOLIT    152          57         230      3     40        0        0   \n",
       "B-LOC          96          15           4    347    116        0        1   \n",
       "I-LOC         202           3          25     20   1109        0        1   \n",
       "B-MEDIA        70           0           0      2      0      159       43   \n",
       "I-MEDIA       107           0           4      1      3       14      427   \n",
       "B-PER         313           9           3     11     14        1        5   \n",
       "I-PER         687           5           7      1     20        3        3   \n",
       "B-ORG         358          10           1     25      1        4        4   \n",
       "I-ORG         667          12           7      9     72        1       23   \n",
       "\n",
       "            B-PER  I-PER  B-ORG  I-ORG  \n",
       "O             286    983    240    692  \n",
       "B-GEOPOLIT     19      6     19     48  \n",
       "I-GEOPOLIT      5     35      1     60  \n",
       "B-LOC          21     11     11     15  \n",
       "I-LOC           8     30      4     39  \n",
       "B-MEDIA         4      0     14      3  \n",
       "I-MEDIA        10     17      1     51  \n",
       "B-PER        1343    498     14     23  \n",
       "I-PER         246   5550      7     39  \n",
       "B-ORG          18      8    673    245  \n",
       "I-ORG          32     93    100   2395  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_output = post_mlm_trainer.predict(ds_tokenized_test)\n",
    "logits = predictions_output.predictions\n",
    "labels = predictions_output.label_ids\n",
    "preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "pred_labels = [[label_names[p] for (p, l) in zip(pred, label) if l != -100] for pred, label in zip(preds, labels)]\n",
    "\n",
    "y_true_flat = [l for seq in true_labels for l in seq]\n",
    "y_pred_flat = [l for seq in pred_labels for l in seq]\n",
    "\n",
    "cm = confusion_matrix(y_true_flat, y_pred_flat, labels=label_names)\n",
    "cm_df = pd.DataFrame(cm, index=label_names, columns=label_names)\n",
    "print(\"Confusion Matrix:\")\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0c08181c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    GEOPOLIT       0.41      0.42      0.42       698\n",
      "         LOC       0.37      0.39      0.38       637\n",
      "       MEDIA       0.36      0.37      0.36       295\n",
      "         ORG       0.26      0.28      0.27      1347\n",
      "         PER       0.36      0.40      0.38      2234\n",
      "\n",
      "   micro avg       0.34      0.37      0.35      5211\n",
      "   macro avg       0.35      0.37      0.36      5211\n",
      "weighted avg       0.34      0.37      0.35      5211\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0498d0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.9038508, 'word': 'Барак Обама', 'start': 0, 'end': 11}, {'entity_group': 'PER', 'score': 0.40524822, 'word': 'Ю', 'start': 33, 'end': 34}, {'entity_group': 'GEOPOLIT', 'score': 0.64348817, 'word': '##ты', 'start': 34, 'end': 36}, {'entity_group': 'GEOPOLIT', 'score': 0.89571035, 'word': 'США', 'start': 44, 'end': 47}, {'entity_group': 'GEOPOLIT', 'score': 0.86792535, 'word': 'Китае', 'start': 50, 'end': 55}, {'entity_group': 'GEOPOLIT', 'score': 0.86807376, 'word': 'США', 'start': 66, 'end': 69}, {'entity_group': 'PER', 'score': 0.82137775, 'word': 'Барак Обама 16', 'start': 70, 'end': 84}, {'entity_group': 'PER', 'score': 0.48506346, 'word': 'Ю', 'start': 116, 'end': 117}, {'entity_group': 'LOC', 'score': 0.32584092, 'word': '##та', 'start': 117, 'end': 119}, {'entity_group': 'PER', 'score': 0.9545311, 'word': 'Джона Хантсмена - младшего ( John Huntsman Jr. )', 'start': 120, 'end': 164}, {'entity_group': 'GEOPOLIT', 'score': 0.6635784, 'word': 'США', 'start': 172, 'end': 175}, {'entity_group': 'GEOPOLIT', 'score': 0.42280126, 'word': 'в', 'start': 176, 'end': 177}, {'entity_group': 'GEOPOLIT', 'score': 0.6747036, 'word': 'Китае, пишет', 'start': 178, 'end': 190}, {'entity_group': 'MEDIA', 'score': 0.55904496, 'word': 'The', 'start': 191, 'end': 194}, {'entity_group': 'MEDIA', 'score': 0.7877095, 'word': 'Washington Times.', 'start': 195, 'end': 212}, {'entity_group': 'GEOPOLIT', 'score': 0.4028673, 'word': 'Белого', 'start': 237, 'end': 243}, {'entity_group': 'LOC', 'score': 0.43784845, 'word': 'дома', 'start': 244, 'end': 248}, {'entity_group': 'GEOPOLIT', 'score': 0.55709094, 'word': ',', 'start': 248, 'end': 249}, {'entity_group': 'PER', 'score': 0.33332294, 'word': 'кандидатуру', 'start': 250, 'end': 261}, {'entity_group': 'PER', 'score': 0.93281513, 'word': 'Хантсмена Обаме предложил', 'start': 262, 'end': 287}, {'entity_group': 'PER', 'score': 0.9645627, 'word': 'Джефф Бэйдер ( Jeff Bader ), который', 'start': 319, 'end': 353}, {'entity_group': 'ORG', 'score': 0.90661544, 'word': 'Республиканской партии, в том', 'start': 524, 'end': 553}, {'entity_group': 'PER', 'score': 0.6620765, 'word': 'на выборах', 'start': 584, 'end': 594}, {'entity_group': 'PER', 'score': 0.8868788, 'word': 'Маккейн, поприветствовали', 'start': 615, 'end': 640}, {'entity_group': 'PER', 'score': 0.6864847, 'word': 'Обамы. Они, однако', 'start': 647, 'end': 665}, {'entity_group': 'PER', 'score': 0.579375, 'word': 'Хантсмен фактически лишился', 'start': 722, 'end': 749}, {'entity_group': 'PER', 'score': 0.59062254, 'word': 'был одним', 'start': 859, 'end': 868}, {'entity_group': 'PER', 'score': 0.9156436, 'word': 'Маккейна и, по оценкам', 'start': 898, 'end': 920}, {'entity_group': 'PER', 'score': 0.83288723, 'word': '##мена', 'start': 1051, 'end': 1055}, {'entity_group': 'PER', 'score': 0.44778165, 'word': 'послом', 'start': 1056, 'end': 1062}, {'entity_group': 'PER', 'score': 0.56508464, 'word': 'устранил', 'start': 1070, 'end': 1078}, {'entity_group': 'PER', 'score': 0.66841, 'word': 'работал', 'start': 1162, 'end': 1169}, {'entity_group': 'GEOPOLIT', 'score': 0.49112284, 'word': 'Джордже', 'start': 1202, 'end': 1209}, {'entity_group': 'PER', 'score': 0.83621544, 'word': 'Буше - младшем,', 'start': 1210, 'end': 1223}, {'entity_group': 'GEOPOLIT', 'score': 0.7073036, 'word': 'Сингапуре', 'start': 1252, 'end': 1261}]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<mark style=\"background-color: pink\">Барак Обама<sub>(PER)</sub></mark> назначил губернатора <mark style=\"background-color: pink\">Ю<sub>(PER)</sub></mark><mark style=\"background-color: yellow\">ты<sub>(GEOPOLIT)</sub></mark> послом <mark style=\"background-color: yellow\">США<sub>(GEOPOLIT)</sub></mark> в <mark style=\"background-color: yellow\">Китае<sub>(GEOPOLIT)</sub></mark> Президент <mark style=\"background-color: yellow\">США<sub>(GEOPOLIT)</sub></mark> <mark style=\"background-color: pink\">Барак Обама 16<sub>(PER)</sub></mark> мая назначил губернатора штата <mark style=\"background-color: pink\">Ю<sub>(PER)</sub></mark><mark style=\"background-color: lightblue\">та<sub>(LOC)</sub></mark> <mark style=\"background-color: pink\">Джона Хантсмена-младшего (John Huntsman Jr.)<sub>(PER)</sub></mark> послом <mark style=\"background-color: yellow\">США<sub>(GEOPOLIT)</sub></mark> <mark style=\"background-color: yellow\">в<sub>(GEOPOLIT)</sub></mark> <mark style=\"background-color: yellow\">Китае, пишет<sub>(GEOPOLIT)</sub></mark> <mark style=\"background-color: lightgreen\">The<sub>(MEDIA)</sub></mark> <mark style=\"background-color: lightgreen\">Washington Times.<sub>(MEDIA)</sub></mark> По словам представителя <mark style=\"background-color: yellow\">Белого<sub>(GEOPOLIT)</sub></mark> <mark style=\"background-color: lightblue\">дома<sub>(LOC)</sub></mark><mark style=\"background-color: yellow\">,<sub>(GEOPOLIT)</sub></mark> <mark style=\"background-color: pink\">кандидатуру<sub>(PER)</sub></mark> <mark style=\"background-color: pink\">Хантсмена Обаме предложил<sub>(PER)</sub></mark> помощник по азиатской политике <mark style=\"background-color: pink\">Джефф Бэйдер (Jeff Bader), который<sub>(PER)</sub></mark> представил губернатора Юты как человека, отлично знающего китайский язык, разбирающегося в проблемах региона и способного эффективно решать дипломатические задачи. Члены <mark style=\"background-color: lightgrey\">Республиканской партии, в том<sub>(ORG)</sub></mark> числе и бывший соперник Обамы <mark style=\"background-color: pink\">на выборах<sub>(PER)</sub></mark> президента США Джон <mark style=\"background-color: pink\">Маккейн, поприветствовали<sub>(PER)</sub></mark> выбор <mark style=\"background-color: pink\">Обамы. Они, однако<sub>(PER)</sub></mark>, в то же время признали, что в связи с этим назначением <mark style=\"background-color: pink\">Хантсмен фактически лишился<sub>(PER)</sub></mark> возможности участвовать в следующих президентских выборах. Во время предвыборной кампании 2008 года Хантсмен <mark style=\"background-color: pink\">был одним<sub>(PER)</sub></mark> из руководителей штаба Джона <mark style=\"background-color: pink\">Маккейна и, по оценкам<sub>(PER)</sub></mark> экспертов, именно он мог стать кандидатом от республиканцев на следующих выборах. Многие республиканцы отметили, что, сделав Хантс<mark style=\"background-color: pink\">мена<sub>(PER)</sub></mark> <mark style=\"background-color: pink\">послом<sub>(PER)</sub></mark>, Обама <mark style=\"background-color: pink\">устранил<sub>(PER)</sub></mark> потенциально опасного соперника. До избрания губернатором Юты в 2004 году Хантсмен <mark style=\"background-color: pink\">работал<sub>(PER)</sub></mark> торговым представителем США при <mark style=\"background-color: yellow\">Джордже<sub>(GEOPOLIT)</sub></mark> <mark style=\"background-color: pink\">Буше-младшем,<sub>(PER)</sub></mark> а до этого был послом США в <mark style=\"background-color: yellow\">Сингапуре<sub>(GEOPOLIT)</sub></mark>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\", device=0)\n",
    "\n",
    "text = \"Барак Обама назначил губернатора Юты послом США в Китае Президент США Барак Обама 16 мая назначил губернатора штата Юта Джона Хантсмена-младшего (John Huntsman Jr.) послом США в Китае, пишет The Washington Times. По словам представителя Белого дома, кандидатуру Хантсмена Обаме предложил помощник по азиатской политике Джефф Бэйдер (Jeff Bader), который представил губернатора Юты как человека, отлично знающего китайский язык, разбирающегося в проблемах региона и способного эффективно решать дипломатические задачи. Члены Республиканской партии, в том числе и бывший соперник Обамы на выборах президента США Джон Маккейн, поприветствовали выбор Обамы. Они, однако, в то же время признали, что в связи с этим назначением Хантсмен фактически лишился возможности участвовать в следующих президентских выборах. Во время предвыборной кампании 2008 года Хантсмен был одним из руководителей штаба Джона Маккейна и, по оценкам экспертов, именно он мог стать кандидатом от республиканцев на следующих выборах. Многие республиканцы отметили, что, сделав Хантсмена послом, Обама устранил потенциально опасного соперника. До избрания губернатором Юты в 2004 году Хантсмен работал торговым представителем США при Джордже Буше-младшем, а до этого был послом США в Сингапуре.\"\n",
    "entities = ner_pipeline(text)\n",
    "print(entities)\n",
    "\n",
    "predicted_entities = [\n",
    "    Ne5Span(start=ent[\"start\"], stop=ent[\"end\"], type=ent[\"entity_group\"])\n",
    "    for ent in entities\n",
    "]\n",
    "\n",
    "predicted_labeled_text = LabeledText(text=text, entities=predicted_entities)\n",
    "\n",
    "visualize_labeled_text(predicted_labeled_text, COLOR_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c66796",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_mlm_trainer.save_model(\"./post_mlm_model\")\n",
    "del post_mlm_trainer\n",
    "del tokenizer\n",
    "del data_collator\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d408572b",
   "metadata": {},
   "source": [
    "# 5. Использование доп разметки"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ae6816",
   "metadata": {},
   "source": [
    "## 5.1 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "94b58aed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>bio_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Египетский, перевозчик, EgyptAir, сообщил, о,...</td>\n",
       "      <td>[O, O, B-ORG, O, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Глава, Красногорского, района, Московской, об...</td>\n",
       "      <td>[O, B-LOC, I-LOC, B-LOC, I-LOC, B-PER, I-PER, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Депутат, Виталий, Милонов, внес, в, Госдуму, ...</td>\n",
       "      <td>[O, B-PER, I-PER, O, O, B-ORG, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Верховный, суд, Индии, разрешил, женщинам, в,...</td>\n",
       "      <td>[B-ORG, I-ORG, B-LOC, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Россиянам, не, стоит, бояться, роста, цен, на...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               words  \\\n",
       "0  [Египетский, перевозчик, EgyptAir, сообщил, о,...   \n",
       "1  [Глава, Красногорского, района, Московской, об...   \n",
       "2  [Депутат, Виталий, Милонов, внес, в, Госдуму, ...   \n",
       "3  [Верховный, суд, Индии, разрешил, женщинам, в,...   \n",
       "4  [Россиянам, не, стоит, бояться, роста, цен, на...   \n",
       "\n",
       "                                          bio_labels  \n",
       "0  [O, O, B-ORG, O, O, O, O, O, O, O, O, O, O, O,...  \n",
       "1  [O, B-LOC, I-LOC, B-LOC, I-LOC, B-PER, I-PER, ...  \n",
       "2  [O, B-PER, I-PER, O, O, B-ORG, O, O, O, O, O, ...  \n",
       "3  [B-ORG, I-ORG, B-LOC, O, O, O, O, O, O, O, O, ...  \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenta = pd.read_parquet('data/synthetic_annotations.parquet')\n",
    "lenta = lenta.rename(columns={'text': 'words', 'annotation': 'bio_labels'})\n",
    "lenta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b21f74e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee99a70a3e44a7daba7c32a1ad61bb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9713 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "lenta_dataset = Dataset.from_pandas(lenta, preserve_index=False)\n",
    "labels_tokenizer_aligner = LabelsTokenizerAligner(bio_labels_to_idx, tokenizer)\n",
    "dataset_tokenized = lenta_dataset.map(labels_tokenizer_aligner, batched =True, remove_columns=['words', 'bio_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9885dfb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 800\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 9713\n",
       " }))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_tokenized_train, dataset_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fd4b84b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 10513\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "combined_ds = concatenate_datasets([ds_tokenized_train, dataset_tokenized])\n",
    "\n",
    "print(combined_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b5b5801f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "73a4b7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=25,\n",
    "    weight_decay=0.01,\n",
    "    seed=random_state,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=combined_ds,\n",
    "    eval_dataset=ds_tokenized_test,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "90818d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрики до дообучения:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 01:33]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.386500358581543,\n",
       " 'eval_model_preparation_time': 0.0,\n",
       " 'eval_precision': 0.00340171330194354,\n",
       " 'eval_recall': 0.03147188639416619,\n",
       " 'eval_f1': 0.006139792594811126,\n",
       " 'eval_accuracy': 0.08927073299568095,\n",
       " 'eval_runtime': 1.7142,\n",
       " 'eval_samples_per_second': 116.674,\n",
       " 'eval_steps_per_second': 14.584}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Метрики до дообучения:\")\n",
    "trainer.evaluate(ds_tokenized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b2491b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32875' max='32875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32875/32875 36:56, Epoch 25/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.204200</td>\n",
       "      <td>0.894058</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.101039</td>\n",
       "      <td>0.123201</td>\n",
       "      <td>0.111025</td>\n",
       "      <td>0.776699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.133100</td>\n",
       "      <td>0.664287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156402</td>\n",
       "      <td>0.166187</td>\n",
       "      <td>0.161146</td>\n",
       "      <td>0.805736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.110300</td>\n",
       "      <td>0.607886</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.174483</td>\n",
       "      <td>0.181347</td>\n",
       "      <td>0.177849</td>\n",
       "      <td>0.816906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.095000</td>\n",
       "      <td>0.540663</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.198361</td>\n",
       "      <td>0.208981</td>\n",
       "      <td>0.203532</td>\n",
       "      <td>0.829475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.085100</td>\n",
       "      <td>0.564435</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.212177</td>\n",
       "      <td>0.222030</td>\n",
       "      <td>0.216992</td>\n",
       "      <td>0.832551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.072600</td>\n",
       "      <td>0.539629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.223066</td>\n",
       "      <td>0.236807</td>\n",
       "      <td>0.229731</td>\n",
       "      <td>0.837834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.061900</td>\n",
       "      <td>0.526838</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.243942</td>\n",
       "      <td>0.262713</td>\n",
       "      <td>0.252980</td>\n",
       "      <td>0.843629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.055900</td>\n",
       "      <td>0.542800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.255297</td>\n",
       "      <td>0.277490</td>\n",
       "      <td>0.265931</td>\n",
       "      <td>0.844949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.053600</td>\n",
       "      <td>0.569877</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.246544</td>\n",
       "      <td>0.270390</td>\n",
       "      <td>0.257917</td>\n",
       "      <td>0.843753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.045800</td>\n",
       "      <td>0.526747</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.274383</td>\n",
       "      <td>0.292074</td>\n",
       "      <td>0.282952</td>\n",
       "      <td>0.853385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.042100</td>\n",
       "      <td>0.566286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.269891</td>\n",
       "      <td>0.298791</td>\n",
       "      <td>0.283607</td>\n",
       "      <td>0.850884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.041000</td>\n",
       "      <td>0.553074</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.274259</td>\n",
       "      <td>0.299942</td>\n",
       "      <td>0.286526</td>\n",
       "      <td>0.853354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.034800</td>\n",
       "      <td>0.592731</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.277566</td>\n",
       "      <td>0.307235</td>\n",
       "      <td>0.291648</td>\n",
       "      <td>0.851692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.034300</td>\n",
       "      <td>0.572606</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287401</td>\n",
       "      <td>0.312992</td>\n",
       "      <td>0.299651</td>\n",
       "      <td>0.855995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.033400</td>\n",
       "      <td>0.588530</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.286800</td>\n",
       "      <td>0.318557</td>\n",
       "      <td>0.301846</td>\n",
       "      <td>0.855933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>0.614982</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.292113</td>\n",
       "      <td>0.319133</td>\n",
       "      <td>0.305026</td>\n",
       "      <td>0.855436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.029100</td>\n",
       "      <td>0.627153</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.290837</td>\n",
       "      <td>0.322203</td>\n",
       "      <td>0.305717</td>\n",
       "      <td>0.854333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.029100</td>\n",
       "      <td>0.617895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.292542</td>\n",
       "      <td>0.320668</td>\n",
       "      <td>0.305960</td>\n",
       "      <td>0.855762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.026400</td>\n",
       "      <td>0.606242</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.294006</td>\n",
       "      <td>0.325657</td>\n",
       "      <td>0.309023</td>\n",
       "      <td>0.857083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.024800</td>\n",
       "      <td>0.640745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.293347</td>\n",
       "      <td>0.322395</td>\n",
       "      <td>0.307186</td>\n",
       "      <td>0.855560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.026500</td>\n",
       "      <td>0.622962</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.295478</td>\n",
       "      <td>0.326041</td>\n",
       "      <td>0.310008</td>\n",
       "      <td>0.857440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.024600</td>\n",
       "      <td>0.623713</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.302354</td>\n",
       "      <td>0.332758</td>\n",
       "      <td>0.316828</td>\n",
       "      <td>0.858202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.023700</td>\n",
       "      <td>0.637646</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.297654</td>\n",
       "      <td>0.328728</td>\n",
       "      <td>0.312420</td>\n",
       "      <td>0.857223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.024900</td>\n",
       "      <td>0.628273</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.301455</td>\n",
       "      <td>0.333909</td>\n",
       "      <td>0.316853</td>\n",
       "      <td>0.857953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.024800</td>\n",
       "      <td>0.634914</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.301844</td>\n",
       "      <td>0.332950</td>\n",
       "      <td>0.316635</td>\n",
       "      <td>0.857984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=32875, training_loss=0.05902036816267006, metrics={'train_runtime': 2216.6895, 'train_samples_per_second': 118.566, 'train_steps_per_second': 14.831, 'total_flos': 1439291200762776.0, 'train_loss': 0.05902036816267006, 'epoch': 25.0})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a66cc2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрики после дообучения:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6349139213562012,\n",
       " 'eval_model_preparation_time': 0.0,\n",
       " 'eval_precision': 0.30184411969380653,\n",
       " 'eval_recall': 0.33294952984072157,\n",
       " 'eval_f1': 0.31663472944611737,\n",
       " 'eval_accuracy': 0.8579840288350993,\n",
       " 'eval_runtime': 4.9496,\n",
       " 'eval_samples_per_second': 40.408,\n",
       " 'eval_steps_per_second': 5.051,\n",
       " 'epoch': 25.0}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Метрики после дообучения:\")\n",
    "trainer.evaluate(ds_tokenized_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8e9210",
   "metadata": {},
   "source": [
    "## 5.1 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0be2313a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>O</th>\n",
       "      <th>B-GEOPOLIT</th>\n",
       "      <th>I-GEOPOLIT</th>\n",
       "      <th>B-LOC</th>\n",
       "      <th>I-LOC</th>\n",
       "      <th>B-MEDIA</th>\n",
       "      <th>I-MEDIA</th>\n",
       "      <th>B-PER</th>\n",
       "      <th>I-PER</th>\n",
       "      <th>B-ORG</th>\n",
       "      <th>I-ORG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>43487</td>\n",
       "      <td>80</td>\n",
       "      <td>67</td>\n",
       "      <td>198</td>\n",
       "      <td>208</td>\n",
       "      <td>26</td>\n",
       "      <td>70</td>\n",
       "      <td>528</td>\n",
       "      <td>827</td>\n",
       "      <td>424</td>\n",
       "      <td>602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-GEOPOLIT</th>\n",
       "      <td>177</td>\n",
       "      <td>267</td>\n",
       "      <td>54</td>\n",
       "      <td>118</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-GEOPOLIT</th>\n",
       "      <td>242</td>\n",
       "      <td>17</td>\n",
       "      <td>169</td>\n",
       "      <td>3</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>39</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-LOC</th>\n",
       "      <td>66</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>423</td>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-LOC</th>\n",
       "      <td>251</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>42</td>\n",
       "      <td>1056</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>34</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-MEDIA</th>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>103</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-MEDIA</th>\n",
       "      <td>170</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>238</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-PER</th>\n",
       "      <td>304</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1349</td>\n",
       "      <td>547</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-PER</th>\n",
       "      <td>1322</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "      <td>5031</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-ORG</th>\n",
       "      <td>243</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>774</td>\n",
       "      <td>272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-ORG</th>\n",
       "      <td>732</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>47</td>\n",
       "      <td>83</td>\n",
       "      <td>127</td>\n",
       "      <td>2328</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                O  B-GEOPOLIT  I-GEOPOLIT  B-LOC  I-LOC  B-MEDIA  I-MEDIA  \\\n",
       "O           43487          80          67    198    208       26       70   \n",
       "B-GEOPOLIT    177         267          54    118     23        0        0   \n",
       "I-GEOPOLIT    242          17         169      3     85        0        0   \n",
       "B-LOC          66          15           3    423     95        0        0   \n",
       "I-LOC         251           2          20     42   1056        0        1   \n",
       "B-MEDIA        54           1           0      0      0      103       31   \n",
       "I-MEDIA       170           0           3      0      0        6      238   \n",
       "B-PER         304           6           0      3      6        0        0   \n",
       "I-PER        1322           2           3      3     13        3        1   \n",
       "B-ORG         243          10           0     17      3        2        0   \n",
       "I-ORG         732          16          12     17     41        0        8   \n",
       "\n",
       "            B-PER  I-PER  B-ORG  I-ORG  \n",
       "O             528    827    424    602  \n",
       "B-GEOPOLIT     21      9      7     22  \n",
       "I-GEOPOLIT     11     39      7     10  \n",
       "B-LOC          15      9      4      7  \n",
       "I-LOC          10     34      5     20  \n",
       "B-MEDIA         4      0     88     14  \n",
       "I-MEDIA        11     16     30    161  \n",
       "B-PER        1349    547      6     13  \n",
       "I-PER         165   5031     10     15  \n",
       "B-ORG          18      8    774    272  \n",
       "I-ORG          47     83    127   2328  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_output = trainer.predict(ds_tokenized_test)\n",
    "logits = predictions_output.predictions\n",
    "labels = predictions_output.label_ids\n",
    "preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "pred_labels = [[label_names[p] for (p, l) in zip(pred, label) if l != -100] for pred, label in zip(preds, labels)]\n",
    "\n",
    "y_true_flat = [l for seq in true_labels for l in seq]\n",
    "y_pred_flat = [l for seq in pred_labels for l in seq]\n",
    "\n",
    "cm = confusion_matrix(y_true_flat, y_pred_flat, labels=label_names)\n",
    "cm_df = pd.DataFrame(cm, index=label_names, columns=label_names)\n",
    "print(\"Confusion Matrix:\")\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fdd61cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    GEOPOLIT       0.42      0.29      0.34       698\n",
      "         LOC       0.28      0.41      0.33       637\n",
      "       MEDIA       0.34      0.24      0.28       295\n",
      "         ORG       0.24      0.31      0.27      1347\n",
      "         PER       0.32      0.35      0.34      2234\n",
      "\n",
      "   micro avg       0.30      0.33      0.32      5211\n",
      "   macro avg       0.32      0.32      0.31      5211\n",
      "weighted avg       0.31      0.33      0.32      5211\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4c7e811b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.9976202, 'word': 'Барак Обама', 'start': 0, 'end': 11}, {'entity_group': 'PER', 'score': 0.9597403, 'word': 'Юты', 'start': 33, 'end': 36}, {'entity_group': 'GEOPOLIT', 'score': 0.9866236, 'word': 'США', 'start': 44, 'end': 47}, {'entity_group': 'GEOPOLIT', 'score': 0.9596627, 'word': 'Китае', 'start': 50, 'end': 55}, {'entity_group': 'GEOPOLIT', 'score': 0.959941, 'word': 'США', 'start': 66, 'end': 69}, {'entity_group': 'PER', 'score': 0.97175795, 'word': 'Барак Обама 16', 'start': 70, 'end': 84}, {'entity_group': 'LOC', 'score': 0.82748, 'word': 'Юта', 'start': 116, 'end': 119}, {'entity_group': 'PER', 'score': 0.9641198, 'word': 'Джона Хантсмена - младшего ( John Huntsman Jr. )', 'start': 120, 'end': 164}, {'entity_group': 'GEOPOLIT', 'score': 0.8426329, 'word': 'США в', 'start': 172, 'end': 177}, {'entity_group': 'GEOPOLIT', 'score': 0.929965, 'word': 'Китае,', 'start': 178, 'end': 184}, {'entity_group': 'MEDIA', 'score': 0.62432885, 'word': 'The Washington Times.', 'start': 191, 'end': 212}, {'entity_group': 'GEOPOLIT', 'score': 0.92928606, 'word': 'Белого дома,', 'start': 237, 'end': 249}, {'entity_group': 'PER', 'score': 0.9926543, 'word': 'Хантсмена Обаме предложил', 'start': 262, 'end': 287}, {'entity_group': 'PER', 'score': 0.99430597, 'word': 'Джефф Бэйдер ( Jeff Bader ), который', 'start': 319, 'end': 353}, {'entity_group': 'PER', 'score': 0.6040093, 'word': 'как', 'start': 381, 'end': 384}, {'entity_group': 'ORG', 'score': 0.9383564, 'word': 'Республиканской партии, в том', 'start': 524, 'end': 553}, {'entity_group': 'PER', 'score': 0.93008256, 'word': 'Обамы на выборах', 'start': 578, 'end': 594}, {'entity_group': 'GEOPOLIT', 'score': 0.73801756, 'word': 'Джон', 'start': 610, 'end': 614}, {'entity_group': 'PER', 'score': 0.8856569, 'word': 'Маккейн, поприветствовали', 'start': 615, 'end': 640}, {'entity_group': 'PER', 'score': 0.88252157, 'word': 'Обамы. Они', 'start': 647, 'end': 657}, {'entity_group': 'PER', 'score': 0.9269177, 'word': 'Хантсмен фактически', 'start': 722, 'end': 741}, {'entity_group': 'PER', 'score': 0.7665939, 'word': '##мен был одним', 'start': 855, 'end': 868}, {'entity_group': 'PER', 'score': 0.90702164, 'word': 'Маккейна и, по оценкам', 'start': 898, 'end': 920}, {'entity_group': 'PER', 'score': 0.73202807, 'word': '##тсмена послом', 'start': 1049, 'end': 1062}, {'entity_group': 'PER', 'score': 0.9075384, 'word': 'устранил', 'start': 1070, 'end': 1078}, {'entity_group': 'PER', 'score': 0.8868316, 'word': 'работал', 'start': 1162, 'end': 1169}, {'entity_group': 'GEOPOLIT', 'score': 0.5526706, 'word': 'Джордже', 'start': 1202, 'end': 1209}, {'entity_group': 'PER', 'score': 0.92562956, 'word': 'Буше - младшем,', 'start': 1210, 'end': 1223}, {'entity_group': 'GEOPOLIT', 'score': 0.7044088, 'word': 'Сингапуре.', 'start': 1252, 'end': 1262}]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<mark style=\"background-color: pink\">Барак Обама<sub>(PER)</sub></mark> назначил губернатора <mark style=\"background-color: pink\">Юты<sub>(PER)</sub></mark> послом <mark style=\"background-color: yellow\">США<sub>(GEOPOLIT)</sub></mark> в <mark style=\"background-color: yellow\">Китае<sub>(GEOPOLIT)</sub></mark> Президент <mark style=\"background-color: yellow\">США<sub>(GEOPOLIT)</sub></mark> <mark style=\"background-color: pink\">Барак Обама 16<sub>(PER)</sub></mark> мая назначил губернатора штата <mark style=\"background-color: lightblue\">Юта<sub>(LOC)</sub></mark> <mark style=\"background-color: pink\">Джона Хантсмена-младшего (John Huntsman Jr.)<sub>(PER)</sub></mark> послом <mark style=\"background-color: yellow\">США в<sub>(GEOPOLIT)</sub></mark> <mark style=\"background-color: yellow\">Китае,<sub>(GEOPOLIT)</sub></mark> пишет <mark style=\"background-color: lightgreen\">The Washington Times.<sub>(MEDIA)</sub></mark> По словам представителя <mark style=\"background-color: yellow\">Белого дома,<sub>(GEOPOLIT)</sub></mark> кандидатуру <mark style=\"background-color: pink\">Хантсмена Обаме предложил<sub>(PER)</sub></mark> помощник по азиатской политике <mark style=\"background-color: pink\">Джефф Бэйдер (Jeff Bader), который<sub>(PER)</sub></mark> представил губернатора Юты <mark style=\"background-color: pink\">как<sub>(PER)</sub></mark> человека, отлично знающего китайский язык, разбирающегося в проблемах региона и способного эффективно решать дипломатические задачи. Члены <mark style=\"background-color: lightgrey\">Республиканской партии, в том<sub>(ORG)</sub></mark> числе и бывший соперник <mark style=\"background-color: pink\">Обамы на выборах<sub>(PER)</sub></mark> президента США <mark style=\"background-color: yellow\">Джон<sub>(GEOPOLIT)</sub></mark> <mark style=\"background-color: pink\">Маккейн, поприветствовали<sub>(PER)</sub></mark> выбор <mark style=\"background-color: pink\">Обамы. Они<sub>(PER)</sub></mark>, однако, в то же время признали, что в связи с этим назначением <mark style=\"background-color: pink\">Хантсмен фактически<sub>(PER)</sub></mark> лишился возможности участвовать в следующих президентских выборах. Во время предвыборной кампании 2008 года Хантс<mark style=\"background-color: pink\">мен был одним<sub>(PER)</sub></mark> из руководителей штаба Джона <mark style=\"background-color: pink\">Маккейна и, по оценкам<sub>(PER)</sub></mark> экспертов, именно он мог стать кандидатом от республиканцев на следующих выборах. Многие республиканцы отметили, что, сделав Хан<mark style=\"background-color: pink\">тсмена послом<sub>(PER)</sub></mark>, Обама <mark style=\"background-color: pink\">устранил<sub>(PER)</sub></mark> потенциально опасного соперника. До избрания губернатором Юты в 2004 году Хантсмен <mark style=\"background-color: pink\">работал<sub>(PER)</sub></mark> торговым представителем США при <mark style=\"background-color: yellow\">Джордже<sub>(GEOPOLIT)</sub></mark> <mark style=\"background-color: pink\">Буше-младшем,<sub>(PER)</sub></mark> а до этого был послом США в <mark style=\"background-color: yellow\">Сингапуре.<sub>(GEOPOLIT)</sub></mark>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\", device=0)\n",
    "\n",
    "text = \"Барак Обама назначил губернатора Юты послом США в Китае Президент США Барак Обама 16 мая назначил губернатора штата Юта Джона Хантсмена-младшего (John Huntsman Jr.) послом США в Китае, пишет The Washington Times. По словам представителя Белого дома, кандидатуру Хантсмена Обаме предложил помощник по азиатской политике Джефф Бэйдер (Jeff Bader), который представил губернатора Юты как человека, отлично знающего китайский язык, разбирающегося в проблемах региона и способного эффективно решать дипломатические задачи. Члены Республиканской партии, в том числе и бывший соперник Обамы на выборах президента США Джон Маккейн, поприветствовали выбор Обамы. Они, однако, в то же время признали, что в связи с этим назначением Хантсмен фактически лишился возможности участвовать в следующих президентских выборах. Во время предвыборной кампании 2008 года Хантсмен был одним из руководителей штаба Джона Маккейна и, по оценкам экспертов, именно он мог стать кандидатом от республиканцев на следующих выборах. Многие республиканцы отметили, что, сделав Хантсмена послом, Обама устранил потенциально опасного соперника. До избрания губернатором Юты в 2004 году Хантсмен работал торговым представителем США при Джордже Буше-младшем, а до этого был послом США в Сингапуре.\"\n",
    "entities = ner_pipeline(text)\n",
    "print(entities)\n",
    "\n",
    "predicted_entities = [\n",
    "    Ne5Span(start=ent[\"start\"], stop=ent[\"end\"], type=ent[\"entity_group\"])\n",
    "    for ent in entities\n",
    "]\n",
    "\n",
    "predicted_labeled_text = LabeledText(text=text, entities=predicted_entities)\n",
    "\n",
    "visualize_labeled_text(predicted_labeled_text, COLOR_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f75dfb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./ner_finetuned_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2bad47",
   "metadata": {},
   "source": [
    "- MLM-предтренировка перед NER дала наилучшие метрики: F1 (0.3535) и чуть более высокую точность и полноту по сравнению с прямым fine‑tuning без MLM (0.3466) и с fine‑tuning с синтетикой.\n",
    "- Добавление синтетических аннотаций ухудшило все ключевые метрики (F1 упало до 0.3166), вероятно из‑за низкого качества или несоответствия синтетики реальному распределению. В особенности из-за того, что в синтетике не присутствовали теги GEOPOLIt и MEDIA, из-за чего показатели упали для типов сущностей.\n",
    "\n",
    "| Approach        | Precision  | Recall     | F1         | Accuracy   |\n",
    "| --------------- | ----------:| ----------:| ----------:| ----------:|\n",
    "| Baseline FT     | 0.33233533 | 0.36211860 | 0.34658830 | 0.87219961 |\n",
    "| MLM FT          | 0.34106315 | 0.36691614 | 0.35351761 | 0.87474754 |\n",
    "| With Synthetic  | 0.30184412 | 0.33294953 | 0.31663473 | 0.85798403 |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
